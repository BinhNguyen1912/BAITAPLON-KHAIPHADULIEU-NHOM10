# -*- coding: utf-8 -*-
"""HOUSE_PRICES_ADVANCED_REGRESSION_TECHNIQUES_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZzevrwy_IidjwYTcrDn8CR-mpGyjoFx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import os
sns.set_style('whitegrid')

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

train_df.set_index('Id', inplace=True)
test_df.set_index('Id', inplace=True)

print(f"Training data shape: {train_df.shape}")
print(f"Testing data shape: {test_df.shape}")


before_stats = {
    'train_shape': train_df.shape,
    'test_shape': test_df.shape,
    'saleprice_skew': train_df['SalePrice'].skew(),
    'saleprice_mean': train_df['SalePrice'].mean(),
    'saleprice_std': train_df['SalePrice'].std(),
    'total_missing': train_df.isnull().sum().sum() + test_df.isnull().sum().sum(),
    'train_missing': train_df.isnull().sum().sum(),
    'test_missing': test_df.isnull().sum().sum(),
    'n_features': train_df.shape[1] - 1,  # Trừ SalePrice
}
original_SalePrice = train_df['SalePrice'].copy()
train_missing_before = train_df.isnull().sum()
test_missing_before = test_df.isnull().sum()
all_missing_before = pd.concat([train_df, test_df]).isnull().sum()
all_missing_before = all_missing_before[all_missing_before > 0].sort_values(ascending=False)

print("\n=== THÔNG TIN CHI TIẾT VỀ DATASET ===")
print("Training data info:")
print(train_df.info())

print("\nCác loại dữ liệu trong training set:")
print(train_df.dtypes.value_counts())

print("\nStatistical summary của numerical features:")
print(train_df.describe())

# Kiểm tra duplicate
print(f"\nSố lượng duplicate trong train: {train_df.duplicated().sum()}")
print(f"Số lượng duplicate trong test: {test_df.duplicated().sum()}")

train_df.head()

"""# I. EDA

## 2.1 Phân phối target variable
"""

plt.figure(figsize=(18, 8))

# --- PHÂN PHỐI BAN ĐẦU (Trước Log) ---
plt.subplot(1, 3, 1)
sns.histplot(train_df['SalePrice'], kde=True, bins=50, color='red')
plt.title('Phân phối SalePrice (Ban đầu)', fontsize=14)
plt.xlabel('Giá Nhà')
plt.ylabel('Tần Suất')
plt.grid(axis='y', alpha=0.5)

# Skewness ban đầu
skew_before = train_df['SalePrice'].skew()
plt.text(0.7, 0.9, f'Skewness ban đầu: {skew_before:.2f}', transform=plt.gca().transAxes,
         fontsize=11, verticalalignment='top', color='darkred', weight='bold')

# --- THỐNG KÊ (Ban đầu) ---
plt.subplot(1, 3, 2)
stats_box = f"""
Tóm tắt Thống kê (Ban đầu):
Trung bình: ${train_df['SalePrice'].mean():,.0f}
Trung vị: ${train_df['SalePrice'].median():,.0f}
Độ lệch chuẩn: ${train_df['SalePrice'].std():,.0f}
Giá trị nhỏ nhất: ${train_df['SalePrice'].min():,.0f}
Giá trị lớn nhất: ${train_df['SalePrice'].max():,.0f}
Độ lệch (Skewness): {train_df['SalePrice'].skew():.2f} (Lệch dương mạnh)
Độ nhọn (Kurtosis): {train_df['SalePrice'].kurtosis():.2f}
"""
plt.text(0.05, 0.95, stats_box, transform=plt.gca().transAxes, fontsize=11,
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='linen', alpha=0.8))
plt.axis('off')

# --- PHÂN PHỐI SAU BIẾN ĐỔI LOG ---
plt.subplot(1, 3, 3)

# Tạo biến đổi log
log_saleprice = np.log1p(train_df['SalePrice'])  # log(1 + x) để tránh lỗi với giá trị 0

sns.histplot(log_saleprice, kde=True, bins=50, color='green')
plt.title('Phân phối SalePrice (Sau Log Transformation)', fontsize=14)
plt.xlabel('Log(1 + Giá Nhà)')
plt.ylabel('Tần Suất')
plt.grid(axis='y', alpha=0.5)

# Tính skewness sau biến đổi log
skew_after = log_saleprice.skew()  #tính skewness của dữ liệu đã biến đổi
plt.text(0.7, 0.9, f'Skewness sau Log: {skew_after:.2f}', transform=plt.gca().transAxes,
         fontsize=11, verticalalignment='top', color='darkgreen', weight='bold')

plt.suptitle('SO SÁNH PHÂN PHỐI SALEPRICE TRƯỚC VÀ SAU BIẾN ĐỔI LOG', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

"""## 2.2 Phân tích outliers trong SalePrice"""

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(y=train_df['SalePrice'])
plt.title('Biểu đồ Hộp - Phân phối Giá Nhà')
plt.ylabel('Giá Nhà')

plt.subplot(1, 2, 2)
# Q-Q plot để kiểm tra tính phân phối chuẩn
from scipy import stats
stats.probplot(train_df['SalePrice'], dist="norm", plot=plt)
plt.title('Biểu đồ Q-Q - Giá Nhà')

plt.tight_layout()
plt.show()

"""## 2.3 Phân tích missing values chi tiết"""

def analyze_missing_data(df, dataset_name):
    missing = df.isnull().sum()
    missing_percent = (missing / len(df)) * 100
    missing_df = pd.DataFrame({
        'Missing Count': missing,
        'Missing Percentage': missing_percent
    }).sort_values('Missing Count', ascending=False)

    missing_df = missing_df[missing_df['Missing Count'] > 0]

    if len(missing_df) > 0:
        print(f"\n=== MISSING VALUES IN {dataset_name.upper()} ===")
        print(f"Tổng số features có missing values: {len(missing_df)}")
        print(f"Tổng số missing values: {missing.sum()}")

        plt.figure(figsize=(12, 8))
        missing_df_head = missing_df.head(20)
        sns.barplot(x=missing_df_head['Missing Percentage'], y=missing_df_head.index)
        plt.title(f'Top 20 Features với Missing Values - {dataset_name}')
        plt.xlabel('Percentage Missing (%)')
        plt.tight_layout()
        plt.show()

        return missing_df
    else:
        print(f"\nNo missing values in {dataset_name}")
        return None

    # Analyze missing values cho cả train và test
missing_train = analyze_missing_data(train_df, "Training Set")
missing_test = analyze_missing_data(test_df, "Test Set")

"""## 2.4 Phân tích correlation matrix mở rộng"""

print("\n=== PHÂN TÍCH TƯƠNG QUAN ===")

# Tính ma trận tương quan
corr_matrix = train_df.select_dtypes(include=[np.number]).corr()

# Tìm top features tương quan với SalePrice
top_corr_with_target = corr_matrix['SalePrice'].abs().sort_values(ascending=False).head(15)
print("Top 15 đặc trưng tương quan với Giá Nhà:")
print(top_corr_with_target)

# Trực quan hóa ma trận tương quan
plt.figure(figsize=(16, 14))
top_features = top_corr_with_target.index
sns.heatmap(corr_matrix.loc[top_features, top_features],
            annot=True, cmap='RdBu_r', center=0,
            fmt='.2f', linewidths=0.5)
plt.title('Ma trận Tương quan - Top 15 Đặc trưng với Giá Nhà')
plt.tight_layout()
plt.show()

"""## 2.5 Phân tích relationship của top features với SalePrice"""

top_5_features = top_corr_with_target.index[1:6]  # Bỏ Giá Nhà

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, feature in enumerate(top_5_features):
    if feature in train_df.columns:
        axes[i].scatter(train_df[feature], train_df['SalePrice'], alpha=0.6)
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel('Giá Nhà')
        axes[i].set_title(f'{feature} so với Giá Nhà\nTương quan: {corr_matrix.loc[feature, "SalePrice"]:.2f}')

# Ẩn subplot còn lại
for i in range(len(top_5_features), 6):
    axes[i].set_visible(False)

plt.tight_layout()
plt.show()

"""## 2.6 phân tích categorical features quan trọng"""

# PHÂN TÍCH CATEGORICAL FEATURES QUAN TRỌNG
print("=== PHÂN TÍCH CATEGORICAL FEATURES ===")

categorical_features = ['MSZoning', 'Neighborhood', 'HouseStyle', 'RoofStyle', 'HeatingQC']

fig, axes = plt.subplots(2, 3, figsize=(20, 12))
axes = axes.ravel()

for i, feature in enumerate(categorical_features):
    if i < len(axes):
        # Boxplot để so sánh distribution
        data_to_plot = train_df[~train_df[feature].isna()]
        sns.boxplot(x=feature, y='SalePrice', data=data_to_plot, ax=axes[i])
        axes[i].set_title(f'{feature} vs SalePrice')
        axes[i].tick_params(axis='x', rotation=45)

# Ẩn subplot thừa
for i in range(len(categorical_features), 6):
    axes[i].set_visible(False)

plt.tight_layout()
plt.show()

# Phân tích thống kê
for feature in categorical_features:
    if feature in train_df.columns:
        print(f"\n--- {feature} ---")
        print(f"Số lượng categories: {train_df[feature].nunique()}")
        print("Giá trung bình theo category:")
        price_by_cat = train_df.groupby(feature)['SalePrice'].agg(['mean', 'median', 'count'])
        print(price_by_cat.sort_values('mean', ascending=False))

"""## 2.7 Phân tích xu hướng giá theo năm"""

# PHÂN TÍCH XU HƯỚNG GIÁ THEO NĂM
print("=== PHÂN TÍCH TEMPORAL TRENDS ===")

fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Giá nhà theo năm xây dựng
axes[0,0].scatter(train_df['YearBuilt'], train_df['SalePrice'], alpha=0.6)
axes[0,0].set_xlabel('Năm Xây Dựng')
axes[0,0].set_ylabel('Giá Nhà')
axes[0,0].set_title('Giá Nhà theo Năm Xây Dựng')

# Giá nhà theo năm remodel
axes[0,1].scatter(train_df['YearRemodAdd'], train_df['SalePrice'], alpha=0.6, color='green')
axes[0,1].set_xlabel('Năm Remodel')
axes[0,1].set_ylabel('Giá Nhà')
axes[0,1].set_title('Giá Nhà theo Năm Remodel')

# Giá nhà theo năm bán
yearly_avg = train_df.groupby('YrSold')['SalePrice'].mean()
axes[1,0].plot(yearly_avg.index, yearly_avg.values, marker='o', linewidth=2)
axes[1,0].set_xlabel('Năm Bán')
axes[1,0].set_ylabel('Giá Trung Bình')
axes[1,0].set_title('Xu Hướng Giá Nhà Theo Năm')

# Tuổi ngôi nhà khi bán
train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']
axes[1,1].scatter(train_df['HouseAge'], train_df['SalePrice'], alpha=0.6, color='red')
axes[1,1].set_xlabel('Tuổi Ngôi Nhà')
axes[1,1].set_ylabel('Giá Nhà')
axes[1,1].set_title('Giá Nhà theo Tuổi Ngôi Nhà')

plt.tight_layout()
plt.show()

# Phân tích thống kê temporal
print("\nPhân tích thống kê theo năm:")
print(f"Giá trung bình theo năm bán:\n{train_df.groupby('YrSold')['SalePrice'].agg(['mean', 'median', 'count'])}")

"""# II. DATA PREPROCESSING & FEATURE ENGINEERING

##**3-DATA PREPROCESSING**

### 3.1 Log transformation cho SalePrice
"""

print(f"Độ lệch của SalePrice trước biến đổi: {train_df['SalePrice'].skew():.4f}")

# Áp dụng biến đổi log
saleprice_before_transform = train_df['SalePrice'].copy()
train_df['SalePrice'] = np.log1p(train_df['SalePrice'])
print(f"Độ lệch của SalePrice sau biến đổi: {train_df['SalePrice'].skew():.4f}")

# Trực quan hóa sau biến đổi
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(train_df['SalePrice'], kde=True, bins=50, color='green')
plt.title('Phân Phối SalePrice Sau Khi Biến Đổi Log')
plt.xlabel('Log(Giá Nhà)')
plt.ylabel('Tần Suất')

plt.subplot(1, 2, 2)
stats.probplot(train_df['SalePrice'], dist="norm", plot=plt)
plt.title('Biểu Đồ Q-Q Sau Khi Biến Đổi Log')

plt.tight_layout()
plt.show()

# Thống kê so sánh
stats_comparison = pd.DataFrame({
    'Trước_Xử_Lý': [
        original_SalePrice.skew(),
        original_SalePrice.kurtosis(),
        original_SalePrice.mean(),
        original_SalePrice.median(),
        original_SalePrice.std()
    ],
    'Sau_Xử_Lý': [
        train_df['SalePrice'].skew(),
        train_df['SalePrice'].kurtosis(),
        train_df['SalePrice'].mean(),
        train_df['SalePrice'].median(),
        train_df['SalePrice'].std()
    ]
}, index=['Độ_lệch', 'Độ_nhọn', 'Trung_bình', 'Trung_vị', 'Độ_lệch_chuẩn'])

print("Thống kê SalePrice:")
print(stats_comparison.round(4))

# Biểu đồ so sánh
plt.figure(figsize=(15, 10))

# Phân phối trước và sau
plt.subplot(2, 3, 1)
sns.histplot(original_SalePrice, kde=True, color='red', alpha=0.6, label='Trước')
sns.histplot(np.expm1(train_df['SalePrice']), kde=True, color='blue', alpha=0.6, label='Sau')
plt.title('Phân Phối SalePrice\nTrước & Sau Xử Lý')
plt.xlabel('SalePrice')
plt.ylabel('Tần Suất')
plt.legend()

# Q-Q plot trước
plt.subplot(2, 3, 2)
stats.probplot(original_SalePrice, dist="norm", plot=plt)
plt.title('Q-Q Plot Trước Log Transform')

# Q-Q plot sau
plt.subplot(2, 3, 3)
stats.probplot(train_df['SalePrice'], dist="norm", plot=plt)
plt.title('Q-Q Plot Sau Log Transform')

# Box plot so sánh
plt.subplot(2, 3, 4)
data_to_plot = [original_SalePrice, np.expm1(train_df['SalePrice'])]
plt.boxplot(data_to_plot, labels=['Trước', 'Sau'])
plt.title('Box Plot So Sánh\nTrước & Sau Xử Lý')
plt.ylabel('SalePrice')

# Độ lệch so sánh
plt.subplot(2, 3, 5)
skew_values = [original_SalePrice.skew(), train_df['SalePrice'].skew()]
plt.bar(['Trước', 'Sau'], skew_values, color=['red', 'blue'], alpha=0.7)
plt.title('So Sánh Độ Lệch (Skewness)')
plt.ylabel('Giá Trị Độ Lệch')
for i, v in enumerate(skew_values):
    plt.text(i, v + 0.05, f'{v:.3f}', ha='center', va='bottom')

# Phần trăm thay đổi
plt.subplot(2, 3, 6)
percent_change = ((stats_comparison['Sau_Xử_Lý'] - stats_comparison['Trước_Xử_Lý']) /
                  stats_comparison['Trước_Xử_Lý'] * 100)
plt.barh(percent_change.index, percent_change.values, color='green', alpha=0.7)
plt.title('Phần Trăm Thay Đổi\n(Sau so với Trước)')
plt.xlabel('Phần Trăm Thay Đổi (%)')
for i, v in enumerate(percent_change.values):
    plt.text(v + (1 if v > 0 else -5), i, f'{v:.1f}%', va='center')

plt.tight_layout()
plt.show()

"""### 3.2 Kiểm tra outliers trong numerical features"""

def detect_outliers(df, feature):
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
    return len(outliers)

print("\n=== PHÂN TÍCH GIÁ TRỊ NGOẠI LAI ===")
numerical_features = train_df.select_dtypes(include=[np.number]).columns
outlier_summary = []

for feature in numerical_features:
    if feature != 'SalePrice':  # Loại trừ biến mục tiêu
        outlier_count = detect_outliers(train_df, feature)
        if outlier_count > 0:
            outlier_summary.append({
                'Đặc trưng': feature,
                'Số lượng ngoại lai': outlier_count,
                'Phần trăm': (outlier_count / len(train_df)) * 100
            })

outlier_df = pd.DataFrame(outlier_summary).sort_values('Số lượng ngoại lai', ascending=False)
print("Top đặc trưng có giá trị ngoại lai:")
print(outlier_df.head(10))

"""### 3.3 Kết hợp dữ liệu train và test"""

# Kết hợp dữ liệu train và test để tiền xử lý đồng nhất
all_data = pd.concat((train_df.loc[:,:'SaleCondition'],
                      test_df.loc[:,:'SaleCondition']))

print(f"Kích thước dữ liệu kết hợp: {all_data.shape}")

# ===PHÂN TÍCH ĐẶC TRƯNG TRƯỚC KHI IMPUTE ===
print("Các đặc trưng phân loại:")
categorical_cols = all_data.select_dtypes(include=['object']).columns
print(f"Số lượng đặc trưng phân loại: {len(categorical_cols)}")
print("Danh sách:", list(categorical_cols))

print("\nCác đặc trưng số:")
numerical_cols = all_data.select_dtypes(include=[np.number]).columns
print(f"Số lượng đặc trưng số: {len(numerical_cols)}")

# Phân tích giá trị duy nhất trong các đặc trưng phân loại
print("\n=== GIÁ TRỊ DUY NHẤT TRONG ĐẶC TRƯNG PHÂN LOẠI ===")
for col in categorical_cols[:10]:  # Hiển thị 10 đặc trưng đầu
    unique_vals = all_data[col].nunique()
    print(f"{col}: {unique_vals} giá trị duy nhất")
    if unique_vals < 10:  # Chỉ hiển thị giá trị nếu ít categories
        print(f"   Các giá trị: {all_data[col].unique()}")

"""### 3.4 Tìm các giá trị bị thiếu trong tập dữ liệu all_data hiện tại"""

# 3.4 Tìm các giá trị bị thiếu trong tập dữ liệu all_data hiện tại
missing_data = all_data.isnull().sum().sort_values(ascending=False)
missing_data = missing_data[missing_data > 0]

print("Các đặc trưng có giá trị bị thiếu:")
print(missing_data)

"""### 3.5 Phân loại missing values theo từng nhóm"""

# Phân loại missing values theo từng nhóm
missing_analysis = pd.DataFrame({
    'Missing_Count': missing_data,
    'Missing_Percentage': (missing_data / len(all_data)) * 100,
    'Data_Type': [all_data[col].dtype for col in missing_data.index]
}).sort_values('Missing_Percentage', ascending=False)

print("Chi tiết missing values:")
print(missing_analysis)

# Trực quan hóa pattern missing values
plt.figure(figsize=(14, 10))
missing_plot_data = missing_analysis.head(25)  # Top 25 features có missing values nhiều nhất

plt.subplot(2, 1, 1)
sns.barplot(x=missing_plot_data['Missing_Percentage'], y=missing_plot_data.index)
plt.title('Top 25 Features với Missing Values (Percentage)')
plt.xlabel('Percentage Missing (%)')

plt.subplot(2, 1, 2)
sns.heatmap(all_data[missing_plot_data.index].isnull(),
            cbar=True, yticklabels=False, cmap='viridis',
            cbar_kws={'label': 'Missing Value'})
plt.title('Missing Values Pattern - Heatmap')
plt.tight_layout()
plt.show()

"""### 3.6 Phân nhóm Features để xử lý giá trị bị thiếu"""

# === PHÂN NHÓM FEATURES ĐỂ XỬ LÝ MISSING VALUES ===
print("\n=== PHÂN NHÓM FEATURES CHO IMPUTATION ===")

# Định nghĩa các nhóm features dựa trên domain knowledge
basement_features = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
                    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']

garage_features = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',
                  'GarageCars', 'GarageArea']

pool_fence_features = ['PoolQC', 'Fence', 'MiscFeature']
location_features = ['Alley', 'FireplaceQu']
exterior_features = ['MasVnrType', 'MasVnrArea']
lot_features = ['LotFrontage']
other_features = ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd',
                 'SaleType', 'Utilities', 'Functional', 'MSZoning']

print(f"Basement features: {len(basement_features)} features")
print(f"Garage features: {len(garage_features)} features")
print(f"Pool/Fence features: {len(pool_fence_features)} features")
print(f"Location features: {len(location_features)} features")
print(f"Exterior features: {len(exterior_features)} features")
print(f"Lot features: {len(lot_features)} features")
print(f"Other features: {len(other_features)} features")

"""### 3.7 IMPUTATION PROCESS

### NHÓM 1: Numerical features impute với 0 (cho missing values có nghĩa là "không có")
"""

# NHÓM 1: Numerical features impute với 0 (cho missing values có nghĩa là "không có")
numerical_cols_to_impute_zero = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars', 'GarageArea', 'BsmtFullBath', 'BsmtHalfBath']

#  Kiểm tra trước khi impute
print("Kiểm tra missing values trước khi impute numerical với 0:")
for col in numerical_cols_to_impute_zero:
    if col in all_data.columns:
        missing_before = all_data[col].isnull().sum()
        if missing_before > 0:
            print(f"  {col}: {missing_before} missing values")

# THỰC HIỆN IMPUTE
for col in numerical_cols_to_impute_zero:
    if col in all_data.columns:
        all_data[col] = all_data[col].fillna(0)

#  Kiểm tra sau khi impute
print("Kiểm tra sau khi impute numerical với 0:")
missing_after_zero = all_data[numerical_cols_to_impute_zero].isnull().sum()
print(missing_after_zero)

"""### NHÓM 2: LotFrontage - Impute với median của neighborhood"""

# NHÓM 2: LotFrontage - Impute với median của neighborhood
all_data['LotFrontage'].value_counts()

print(f"Missing values trong LotFrontage trước khi impute: {all_data['LotFrontage'].isnull().sum()}")
print(f"Số lượng neighborhoods: {all_data['Neighborhood'].nunique()}")

if 'LotFrontage' in all_data.columns and all_data['LotFrontage'].isnull().any():
    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

print(f"Missing values trong LotFrontage sau khi impute: {all_data['LotFrontage'].isnull().sum()}")

"""### NHÓM 3: Categorical features impute với 'None' (cho missing values có nghĩa là "không có")"""

# NHÓM 3: Categorical features impute với 'None' (cho missing values có nghĩa là "không có")
categorical_cols_to_impute_none = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']

# Kiểm tra trước khi impute
print("Kiểm tra missing values trước khi impute categorical với 'None':")
for col in categorical_cols_to_impute_none:
    if col in all_data.columns:
        missing_before = all_data[col].isnull().sum()
        if missing_before > 0:
            print(f"  {col}: {missing_before} missing values")

# THỰC HIỆN IMPUTE
for col in categorical_cols_to_impute_none:
    if col in all_data.columns: # Check if column exists after one-hot encoding
        all_data[col] = all_data[col].fillna('None')

# Kiểm tra sau khi impute
print("Kiểm tra sau khi impute categorical với 'None':")
for col in categorical_cols_to_impute_none:
    if col in all_data.columns:
        missing_after = all_data[col].isnull().sum()
        if missing_after > 0:
            print(f"  {col}: VẪN CÒN {missing_after} missing values")
print("Không có!")

"""### NHÓM 4: Categorical features impute với mode (cho missing values thông thường)"""

# NHÓM 4: Categorical features impute với mode (cho missing values thông thường)
mode_impute_cols = ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities', 'Functional', 'MSZoning']

#  Kiểm tra trước khi impute và hiển thị mode
print("Kiểm tra và tìm mode cho các features:")
for col in mode_impute_cols:
    if col in all_data.columns:
        missing_before = all_data[col].isnull().sum()
        mode_value = all_data[col].mode()[0] if not all_data[col].mode().empty else 'Unknown'
        if missing_before > 0:
            print(f"  {col}: {missing_before} missing values, Mode: {mode_value}")

# THỰC HIỆN IMPUTE
for col in ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities', 'Functional', 'MSZoning']:
    if col in all_data.columns: # Check if column exists after one-hot encoding
        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])

#  Kiểm tra sau khi impute
print("Kiểm tra sau khi impute categorical với mode:")
for col in mode_impute_cols:
    if col in all_data.columns:
        missing_after = all_data[col].isnull().sum()
        if missing_after > 0:
            print(f"  {col}: VẪN CÒN {missing_after} missing values")
print("Không có!")

"""### NHÓM 5: Các numerical features còn lại"""

# NHÓM 5: Các đặc trưng số còn lại
# Dựa trên các giá trị thiếu còn lại có khả năng (GarageYrBlt), impute các đặc trưng số còn lại.
# GarageYrBlt có thể được impute với 0 (giả định 0 có nghĩa là không có garage, nhất quán với GarageArea/Cars=0)
if 'GarageYrBlt' in all_data.columns:
    print(f"Giá trị thiếu trong GarageYrBlt trước khi impute: {all_data['GarageYrBlt'].isnull().sum()}")
    all_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)
    print(f"Giá trị thiếu trong GarageYrBlt sau khi impute: {all_data['GarageYrBlt'].isnull().sum()}")

# Kiểm tra tổng thể sau tất cả imputation
print("\n=== KIỂM TRA TỔNG THỂ SAU IMPUTATION ===")
remaining_missing = all_data.isnull().sum()
remaining_missing = remaining_missing[remaining_missing > 0]

if len(remaining_missing) > 0:
    print("VẪN CÒN giá trị thiếu trong các đặc trưng sau:")
    for col, count in remaining_missing.items():
        print(f"  {col}: {count} giá trị thiếu")

    # Xử lý các giá trị thiếu còn lại (nếu có)
    print("\nXử lý các giá trị thiếu còn lại...")
    for col in remaining_missing.index:
        # Convert 'AgeGroup_Num' to numerical before imputation if it exists and is not numerical
        if col == 'AgeGroup_Num' and all_data[col].dtype.name == 'category':
             all_data[col] = all_data[col].cat.codes # Convert to numerical codes
             all_data[col] = all_data[col].replace(-1, np.nan) # Replace -1 (missing) with NaN
             print(f"  {col}: đã chuyển đổi sang số và sẽ điền với trung vị")

        if all_data[col].dtype == 'object':
            # Nếu là phân loại, điền với giá trị phổ biến nhất (mode)
            all_data[col] = all_data[col].fillna(all_data[col].mode()[0] if not all_data[col].mode().empty else 'Không_xác_định')
            print(f"  {col} (phân loại): đã điền với giá trị phổ biến nhất")
        else:
            # Nếu là số, điền với giá trị trung vị (median)
            all_data[col] = all_data[col].fillna(all_data[col].median())
            print(f"  {col} (số): đã điền với giá trị trung vị")

# KIỂM TRA CUỐI CÙNG
print("\nSố lượng giá trị thiếu sau tất cả imputation:", all_data.isnull().sum().sum())

# Xác nhận không còn giá trị thiếu
if all_data.isnull().sum().sum() == 0:
    print("Tất cả giá trị thiếu đã được xử lý!")
else:
    print(" Vẫn còn giá trị thiếu trong dataset")
    # Hiển thị các đặc trưng còn giá trị thiếu
    final_missing = all_data.isnull().sum()
    final_missing = final_missing[final_missing > 0]
    print("Các đặc trưng còn giá trị thiếu:")
    print(final_missing)

# #  Xác định và xử lý giá trị tuổi âm trong cột 'Age'
# # Nếu YrSold < YearBuilt, giả định Age = 0 (nhà mới)
# all_data['Age'] = np.where(all_data['Age'] < 0, 0, all_data['Age'])

# #  Bây giờ bạn có thể an toàn tạo cột AgeGroup_Num
# all_data['AgeGroup_Num'] = pd.cut(all_data['Age'],
#                                  bins=[-1, 5, 15, 30, 50, 100, 200],
#                                  labels=[1, 2, 3, 4, 5, 6])

# # Chuyển đổi sang kiểu số
# all_data['AgeGroup_Num'] = all_data['AgeGroup_Num'].astype(int)

"""## **4-FEATURE ENGINEERING NÂNG CAO**

### 4.1 Xác nhận trạng thái dữ liệu trước khi feature engineering
"""

# === GIAI ĐOẠN 4: FEATURE ENGINEERING NÂNG CAO ===
# 4.1 Xác nhận trạng thái dữ liệu trước khi feature engineering
print("\n=== KIỂM TRA TRẠNG THÁI DỮ LIỆU ===")
print(f"Shape của all_data: {all_data.shape}")
print(f"Tổng số missing values: {all_data.isnull().sum().sum()}")
print(f"Data types:\n{all_data.dtypes.value_counts()}")


n_features_before_engineering = all_data.shape[1]

"""### 4.2 Tạo các features"""

# 4.2 Tạo các features
print("\n=== TẠO CÁC FEATURES CƠ BẢN ===")

# Create a total square footage feature
all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']

# Create a total bathrooms feature
all_data['TotalBath'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +
                         all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))

# Create a feature for age of the house at sale
all_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']

print(" Đã tạo các features cơ bản:")
print(f"   - TotalSF: Tổng diện tích")
print(f"   - TotalBath: Tổng số phòng tắm")
print(f"   - Age: Tuổi của ngôi nhà")

print("\n=== FEATURE ENGINEERING NÂNG CAO ===")

#  Features về chất lượng và điều kiện
print("1. Tạo features về chất lượng và điều kiện")

# Overall quality score (kết hợp chất lượng và điều kiện)
all_data['OverallGrade'] = all_data['OverallQual'] * all_data['OverallCond']

# Quality to condition ratio
all_data['QualCondRatio'] = (all_data['OverallQual'] + 1) / (all_data['OverallCond'] + 1)  # +1 để tránh chia 0

# Exterior quality score (chỉ tạo nếu columns tồn tại)
exterior_qual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}
if 'ExterQual' in all_data.columns:
    all_data['ExterQualScore'] = all_data['ExterQual'].map(exterior_qual_map)
if 'ExterCond' in all_data.columns:
    all_data['ExterCondScore'] = all_data['ExterCond'].map(exterior_qual_map)

#  Features về diện tích và không gian
print("2. Tạo features về diện tích và không gian")

# Total porch area
all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['EnclosedPorch'] +
                           all_data['3SsnPorch'] + all_data['ScreenPorch'])

# Living area ratio (tránh chia 0)
all_data['LivingAreaRatio'] = all_data['GrLivArea'] / (all_data['LotArea'] + 1)

# Room size metrics (tránh chia 0)
all_data['RoomArea'] = all_data['GrLivArea'] / (all_data['TotRmsAbvGrd'] + 1)
all_data['BedroomRatio'] = all_data['BedroomAbvGr'] / (all_data['TotRmsAbvGrd'] + 1)
all_data['BathroomRatio'] = all_data['TotalBath'] / (all_data['TotRmsAbvGrd'] + 1)

#  Features về tiện nghi và facilities
print("3. Tạo features về tiện nghi")

# Has basement flag
all_data['HasBasement'] = (all_data['TotalBsmtSF'] > 0).astype(int)

# Has garage flag
all_data['HasGarage'] = (all_data['GarageArea'] > 0).astype(int)

# Has pool flag
all_data['HasPool'] = (all_data['PoolArea'] > 0).astype(int)

# Has fireplace flag
all_data['HasFireplace'] = (all_data['Fireplaces'] > 0).astype(int)

# Has second floor flag
all_data['HasSecondFloor'] = (all_data['2ndFlrSF'] > 0).astype(int)

# 4 Features về thời gian và remodel

# Remodeled flag
all_data['IsRemodeled'] = (all_data['YearRemodAdd'] != all_data['YearBuilt']).astype(int)

# Years since remodel
all_data['YearsSinceRemodel'] = all_data['YrSold'] - all_data['YearRemodAdd']

# New house flag
all_data['IsNew'] = (all_data['YrSold'] == all_data['YearBuilt']).astype(int)

# Property age groups (sử dụng numerical thay vì categorical để tránh lỗi)
# 4 Features về thời gian và remodel
all_data['AgeGroup_Num'] = pd.cut(all_data['Age'],
                                 bins=[-1, 5, 15, 30, 50, 100, 200],
                                 labels=[1, 2, 3, 4, 5, 6])  # Dùng số thay vì string

#  Features về mùa và thời điểm bán
print("5. Tạo features về mùa bán")

# Season sold (dùng numerical encoding)
def get_season_num(month):
    if month in [12, 1, 2]:
        return 1  # Winter
    elif month in [3, 4, 5]:
        return 2  # Spring
    elif month in [6, 7, 8]:
        return 3  # Summer
    else:
        return 4  # Fall

all_data['SeasonSold_Num'] = all_data['MoSold'].apply(get_season_num)

# Is it summer sale? (thường ảnh hưởng đến giá nhà)
all_data['IsSummerSale'] = (all_data['MoSold'].isin([6, 7, 8])).astype(int)

#  Features tương tác (interaction features)

# Quality per square foot (tránh chia 0)
all_data['QualPerSF'] = all_data['OverallQual'] / (all_data['TotalSF'] + 1)

# Bathroom per bedroom (tránh chia 0)
all_data['BathPerBedroom'] = all_data['TotalBath'] / (all_data['BedroomAbvGr'] + 1)

# Garage cars per area (tránh chia 0)
all_data['GarageCarsPerArea'] = all_data['GarageCars'] / (all_data['GarageArea'] + 1)

# Living area per bedroom (tránh chia 0)
all_data['LivingAreaPerBedroom'] = all_data['GrLivArea'] / (all_data['BedroomAbvGr'] + 1)

#  Features về vị trí và khu vực
# Lot configuration flags
if 'LotConfig' in all_data.columns:
    all_data['IsCornerLot'] = (all_data['LotConfig'] == 'Corner').astype(int)
    all_data['IsCulDSac'] = (all_data['LotConfig'] == 'CulDSac').astype(int)

"""### 4.3 KIỂM TRA VÀ XÁC NHẬN CÁC FEATURES MỚI"""

# 4.3 KIỂM TRA VÀ XÁC NHẬN CÁC FEATURES MỚI
print("\n=== KIỂM TRA CÁC FEATURES MỚI ===")

# Liệt kê tất cả features mới được tạo (CHỈ LẤY NUMERICAL)
new_features_numerical = [
    # Cơ bản
    'TotalSF', 'TotalBath', 'Age',
    # Chất lượng
    'OverallGrade', 'QualCondRatio', 'ExterQualScore', 'ExterCondScore',
    # Diện tích
    'TotalPorchSF', 'LivingAreaRatio', 'RoomArea', 'BedroomRatio', 'BathroomRatio',
    # Tiện nghi
    'HasBasement', 'HasGarage', 'HasPool', 'HasFireplace', 'HasSecondFloor',
    # Thời gian
    'IsRemodeled', 'YearsSinceRemodel', 'IsNew', 'AgeGroup_Num',
    # Mùa bán
    'SeasonSold_Num', 'IsSummerSale',
    # Interaction
    'QualPerSF', 'BathPerBedroom', 'GarageCarsPerArea', 'LivingAreaPerBedroom',
    # Vị trí
    'IsCornerLot', 'IsCulDSac'
]

# Chỉ lấy những features thực sự tồn tại trong all_data
existing_new_features = [f for f in new_features_numerical if f in all_data.columns]

print(f"Đã tạo thành công {len(existing_new_features)} features mới (numerical):")
for i, feature in enumerate(existing_new_features, 1):
    print(f"   {i:2d}. {feature}")

"""### 4.4 PHÂN TÍCH SƠ BỘ CÁC FEATURES MỚI (CHỈ NUMERICAL)"""

print("\n=== PHÂN TÍCH SƠ BỘ CÁC FEATURES MỚI ===")

# Tạo DataFrame tạm thời để phân tích correlation (CHỈ NUMERICAL FEATURES)
temp_df = all_data[:len(train_df)].copy()  # Chỉ lấy phần training data

# Chỉ lấy numerical features để tính correlation
numerical_features_all = temp_df.select_dtypes(include=[np.number]).columns
temp_df_numerical = temp_df[numerical_features_all].copy()
temp_df_numerical['SalePrice'] = train_df['SalePrice']  # Thêm SalePrice đã log-transform

# Tính correlation của các features mới với SalePrice (CHỈ NUMERICAL)
new_features_corr = {}
for feature in existing_new_features:
    if feature in temp_df_numerical.columns:
        try:
            corr = temp_df_numerical[feature].corr(temp_df_numerical['SalePrice'])
            new_features_corr[feature] = corr
        except:
            print(f"    Không thể tính correlation cho {feature}")

# Sắp xếp theo correlation absolute value
new_features_corr_sorted = dict(sorted(new_features_corr.items(),
                                      key=lambda x: abs(x[1]),
                                      reverse=True))

print("Top 15 features mới có correlation cao nhất với SalePrice:")
print("-" * 60)
for i, (feature, corr) in enumerate(list(new_features_corr_sorted.items())[:15], 1):
    correlation_strength = " RẤT CAO" if abs(corr) > 0.5 else " TRUNG BÌNH" if abs(corr) > 0.3 else " THẤP"
    print(f"   {i:2d}. {feature:25s}: {corr:+.4f} {correlation_strength}")

"""### 4.5 VISUALIZATION CÁC FEATURES MỚI QUAN TRỌNG"""

print("\n=== VISUALIZATION FEATURES MỚI QUAN TRỌNG ===")

# Lấy top 6 features mới có correlation cao nhất
top_new_features = list(new_features_corr_sorted.keys())[:6]

if len(top_new_features) > 0:
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()

    for i, feature in enumerate(top_new_features):
        if i < 6:  # Chỉ vẽ 6 subplots
            # Kiểm tra và làm sạch dữ liệu trước khi vẽ
            valid_data = temp_df_numerical[[feature, 'SalePrice']].dropna()

            if len(valid_data) > 0:
                axes[i].scatter(valid_data[feature], valid_data['SalePrice'], alpha=0.6, color='blue')
                axes[i].set_xlabel(feature)
                axes[i].set_ylabel('SalePrice (log)')
                axes[i].set_title(f'{feature} vs SalePrice\nCorr: {new_features_corr[feature]:.3f}')

                # Thêm trend line nếu có đủ điểm
                if len(valid_data) > 1:
                    z = np.polyfit(valid_data[feature], valid_data['SalePrice'], 1)
                    p = np.poly1d(z)
                    axes[i].plot(valid_data[feature], p(valid_data[feature]), "r--", alpha=0.8)

    # Ẩn các subplot không sử dụng
    for i in range(len(top_new_features), 6):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.suptitle('TOP 6 FEATURES MỚI vs SALEPRICE', fontsize=16, y=1.02)
    plt.show()

"""### 4.6 PHÂN TÍCH FEATURE IMPORTANCE BẰNG RANDOM FOREST"""

print("\n=== PHÂN TÍCH FEATURE IMPORTANCE ===")

from sklearn.ensemble import RandomForestRegressor

# Chuẩn bị dữ liệu cho feature importance analysis
X_temp = temp_df_numerical.drop('SalePrice', axis=1).fillna(0)
y_temp = temp_df_numerical['SalePrice']

# Chỉ lấy các features mới để phân tích
new_features_for_importance = [f for f in existing_new_features if f in X_temp.columns]

if len(new_features_for_importance) > 0:
    # Train Random Forest nhanh để lấy feature importance
    rf = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)
    rf.fit(X_temp[new_features_for_importance], y_temp)

    # Lấy feature importance
    feature_importance = pd.DataFrame({
        'feature': new_features_for_importance,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    print("Top 10 features mới quan trọng nhất (theo Random Forest):")
    print("-" * 60)
    for i, row in feature_importance.head(10).iterrows():
        importance_level = " RẤT QUAN TRỌNG" if row['importance'] > 0.05 else " QUAN TRỌNG" if row['importance'] > 0.02 else " BÌNH THƯỜNG"
        print(f"   {i+1:2d}. {row['feature']:25s}: {row['importance']:.4f} {importance_level}")

    # Visualization feature importance
    plt.figure(figsize=(12, 8))
    top_10_importance = feature_importance.head(10)
    sns.barplot(x='importance', y='feature', data=top_10_importance, palette='viridis')
    plt.title('TOP 10 FEATURES MỚI QUAN TRỌNG NHẤT (Random Forest)')
    plt.xlabel('Feature Importance')
    plt.tight_layout()
    plt.show()

    n_features_after_engineering = all_data.shape[1]
    n_new_features = n_features_after_engineering - n_features_before_engineering

"""##TỔNG KẾT , SO SÁNH TRƯỚC VÀ SAU KHI TIỀN XỬ LÝ DỮ LIỆU


"""

after_stats = {
    'all_data_shape': all_data.shape,
    'saleprice_skew': train_df['SalePrice'].skew(),
    'saleprice_mean': train_df['SalePrice'].mean(),
    'saleprice_std': train_df['SalePrice'].std(),
    'total_missing': all_data.isnull().sum().sum(),
    'n_features': all_data.shape[1],
    'n_new_features': n_new_features,
}

print("\nTHỐNG KÊ SAU TIỀN XỬ LÝ:")
print(f"   All data shape: {after_stats['all_data_shape']}")
print(f"   SalePrice skewness: {after_stats['saleprice_skew']:.4f}")
print(f"   SalePrice mean (log): {after_stats['saleprice_mean']:.4f}")
print(f"   SalePrice std (log): {after_stats['saleprice_std']:.4f}")
print(f"   Total missing values: {after_stats['total_missing']}")
print(f"   Number of features: {after_stats['n_features']}")
print(f"   New features created: {after_stats['n_new_features']}")

comparison_df = pd.DataFrame({
    'Metric': [
        'SalePrice Skewness',
        'Total Missing Values',
        'Missing in Train',
        'Missing in Test',
        'Number of Features',
        'Train Rows',
        'Test Rows'
    ],
    'Before': [
        f"{before_stats['saleprice_skew']:.4f}",
        f"{before_stats['total_missing']:,}",
        f"{before_stats['train_missing']:,}",
        f"{before_stats['test_missing']:,}",
        f"{before_stats['n_features']}",
        f"{before_stats['train_shape'][0]:,}",
        f"{before_stats['test_shape'][0]:,}"
    ],
    'After': [
        f"{after_stats['saleprice_skew']:.4f}",
        f"{after_stats['total_missing']}",
        "N/A (merged)",
        "N/A (merged)",
        f"{after_stats['n_features']}",
        "N/A (merged)",
        "N/A (merged)"
    ]
})

print("\nBẢNG SO SÁNH:")
print(comparison_df.to_string(index=False))

# Tính % cải thiện
skew_improvement = ((before_stats['saleprice_skew'] - after_stats['saleprice_skew']) /
                    before_stats['saleprice_skew'] * 100)
missing_improvement = ((before_stats['total_missing'] - after_stats['total_missing']) /
                       before_stats['total_missing'] * 100) if before_stats['total_missing'] > 0 else 0
feature_increase = ((after_stats['n_features'] - before_stats['n_features']) /
                    before_stats['n_features'] * 100)

print("\n CÁC CHỈ SỐ QUAN TRỌNG:")
print(f"   • Skewness giảm: {abs(skew_improvement):.1f}%")
print(f"   • Missing values giảm: {missing_improvement:.1f}%")
print(f"   • Features tăng: {feature_increase:.1f}%")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Before transformation
axes[0, 0].hist(saleprice_before_transform, bins=50, color='red', alpha=0.7, edgecolor='black')
axes[0, 0].set_title('SalePrice - TRƯỚC Log Transform', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Price ($)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].axvline(saleprice_before_transform.mean(), color='blue', linestyle='--',
                   linewidth=2, label=f'Mean: ${saleprice_before_transform.mean():,.0f}')
axes[0, 0].legend()

# After transformation
axes[0, 1].hist(train_df['SalePrice'], bins=50, color='green', alpha=0.7, edgecolor='black')
axes[0, 1].set_title('SalePrice - SAU Log Transform', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Log(Price)')
axes[0, 1].set_ylabel('Frequency')
axes[0, 1].axvline(train_df['SalePrice'].mean(), color='blue', linestyle='--',
                   linewidth=2, label=f'Mean: {train_df["SalePrice"].mean():.2f}')
axes[0, 1].legend()

# Q-Q plot before
stats.probplot(saleprice_before_transform, dist="norm", plot=axes[1, 0])
axes[1, 0].set_title('Q-Q Plot - TRƯỚC Transform', fontsize=14, fontweight='bold')

# Q-Q plot after
stats.probplot(train_df['SalePrice'], dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot - SAU Transform', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('saleprice_transformation_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(16, 8))

# Before
if len(all_missing_before) > 0:
    top_missing_before = all_missing_before.head(15)
    axes[0].barh(range(len(top_missing_before)), top_missing_before.values, color='red', alpha=0.7)
    axes[0].set_yticks(range(len(top_missing_before)))
    axes[0].set_yticklabels(top_missing_before.index)
    axes[0].set_xlabel('Number of Missing Values')
    axes[0].set_title('Top 15 Missing Values - TRƯỚC', fontsize=14, fontweight='bold')
    axes[0].invert_yaxis()

# After
remaining_missing = all_data.isnull().sum()
remaining_missing = remaining_missing[remaining_missing > 0].sort_values(ascending=False)

if len(remaining_missing) > 0:
    axes[1].barh(range(len(remaining_missing)), remaining_missing.values, color='green', alpha=0.7)
    axes[1].set_yticks(range(len(remaining_missing)))
    axes[1].set_yticklabels(remaining_missing.index)
    axes[1].set_xlabel('Number of Missing Values')
    axes[1].set_title('Missing Values - SAU', fontsize=14, fontweight='bold')
    axes[1].invert_yaxis()
else:
    axes[1].text(0.5, 0.5, '✓ NO MISSING VALUES!',
                ha='center', va='center', fontsize=24, color='green', fontweight='bold',
                transform=axes[1].transAxes)
    axes[1].set_title('Missing Values - SAU', fontsize=14, fontweight='bold')
    axes[1].axis('off')

plt.tight_layout()
plt.savefig('missing_values_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# 5.3 Feature Count Comparison
fig, ax = plt.subplots(figsize=(10, 6))
categories = ['Original Features', 'After Engineering']
feature_counts = [before_stats['n_features'], after_stats['n_features']]
colors = ['#FF6B6B', '#4ECDC4']

bars = ax.bar(categories, feature_counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)

# Add value labels on bars
for bar, count in zip(bars, feature_counts):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{count}',
            ha='center', va='bottom', fontsize=16, fontweight='bold')

ax.set_ylabel('Number of Features', fontsize=12)
ax.set_title('Feature Count Comparison', fontsize=16, fontweight='bold')
ax.set_ylim(0, max(feature_counts) * 1.2)

# Add arrow and annotation
arrow_props = dict(arrowstyle='->', lw=2, color='black')
ax.annotate(f'+{n_new_features} features\n(+{feature_increase:.1f}%)',
            xy=(1, feature_counts[1]), xytext=(0.5, feature_counts[1] * 1.1),
            arrowprops=arrow_props, fontsize=12, ha='center',
            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))

plt.tight_layout()
plt.savefig('feature_count_comparison.png', dpi=300, bbox_inches='tight')
plt.show()


print(" DANH SÁCH 26 FEATURES MỚI")

new_features_list = [
    'TotalSF', 'TotalBath', 'Age', 'OverallGrade', 'QualCondRatio',
    'ExterQualScore', 'ExterCondScore', 'TotalPorchSF', 'LivingAreaRatio',
    'RoomArea', 'BedroomRatio', 'BathroomRatio', 'HasBasement', 'HasGarage',
    'HasPool', 'HasFireplace', 'HasSecondFloor', 'IsRemodeled',
    'YearsSinceRemodel', 'IsNew', 'AgeGroup_Num', 'SeasonSold_Num',
    'IsSummerSale', 'QualPerSF', 'BathPerBedroom', 'GarageCarsPerArea',
    'LivingAreaPerBedroom', 'IsCornerLot', 'IsCulDSac'
]

# Lọc chỉ những features thực sự tồn tại
existing_new_features = [f for f in new_features_list if f in all_data.columns]

print(f"\nĐã tạo {len(existing_new_features)} features mới:\n")

categories = {
    'Cơ Bản': ['TotalSF', 'TotalBath', 'Age'],
    'Chất Lượng': ['OverallGrade', 'QualCondRatio', 'ExterQualScore', 'ExterCondScore'],
    'Diện Tích': ['TotalPorchSF', 'LivingAreaRatio', 'RoomArea', 'BedroomRatio', 'BathroomRatio'],
    'Tiện Nghi': ['HasBasement', 'HasGarage', 'HasPool', 'HasFireplace', 'HasSecondFloor'],
    'Thời Gian': ['IsRemodeled', 'YearsSinceRemodel', 'IsNew', 'AgeGroup_Num'],
    'Mùa Bán': ['SeasonSold_Num', 'IsSummerSale'],
    'Tương Tác': ['QualPerSF', 'BathPerBedroom', 'GarageCarsPerArea', 'LivingAreaPerBedroom'],
    'Vị Trí': ['IsCornerLot', 'IsCulDSac']
}

for category, features in categories.items():
    existing = [f for f in features if f in existing_new_features]
    if existing:
        print(f" {category} ({len(existing)} features):")
        for i, feature in enumerate(existing, 1):
            print(f"   {i}. {feature}")
        print()


print(" CORRELATION ANALYSIS - NEW FEATURES")

# Tạo temp dataframe với SalePrice
temp_df = all_data[:len(train_df)].copy()
temp_df['SalePrice'] = train_df['SalePrice']

# Tính correlation cho features mới
new_features_corr = {}
for feature in existing_new_features:
    if feature in temp_df.columns:
        try:
            corr = temp_df[feature].corr(temp_df['SalePrice'])
            if not np.isnan(corr):
                new_features_corr[feature] = corr
        except:
            pass

# Sắp xếp theo absolute correlation
new_features_corr_sorted = dict(sorted(new_features_corr.items(),
                                      key=lambda x: abs(x[1]),
                                      reverse=True))

print(f"\n Top 15 Features Mới có Correlation cao nhất với SalePrice:\n")
print(f"{'Rank':<6}{'Feature':<30}{'Correlation':<15}{'Strength'}")
print("-" * 70)

for i, (feature, corr) in enumerate(list(new_features_corr_sorted.items())[:15], 1):
    if abs(corr) > 0.5:
        strength = "Rất mạnh"
    elif abs(corr) > 0.3:
        strength = "Mạnh"
    elif abs(corr) > 0.1:
        strength = "Vừa phải"
    else:
        strength = "Yếu"

    print(f"{i:<6}{feature:<30}{corr:+.4f}{' '*7}{strength}")

# Visualization correlation
if len(new_features_corr_sorted) > 0:
    fig, ax = plt.subplots(figsize=(12, 8))

    top_features = list(new_features_corr_sorted.items())[:15]
    features = [f[0] for f in top_features]
    corrs = [f[1] for f in top_features]

    colors = ['green' if c > 0 else 'red' for c in corrs]
    bars = ax.barh(features, corrs, color=colors, alpha=0.7, edgecolor='black')

    ax.set_xlabel('Correlation with SalePrice', fontsize=12)
    ax.set_title('Top 15 New Features - Correlation with SalePrice',
                 fontsize=14, fontweight='bold')
    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    ax.invert_yaxis()

    # Add value labels
    for i, (bar, corr) in enumerate(zip(bars, corrs)):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2,
                f' {corr:.3f}',
                ha='left' if corr > 0 else 'right',
                va='center', fontsize=9, fontweight='bold')

    plt.tight_layout()
    plt.savefig('new_features_correlation.png', dpi=300, bbox_inches='tight')
    plt.show()



  # Correlation heatmap for top new features
if len(new_features_corr_sorted) >= 10:
    top_10_features = list(new_features_corr_sorted.keys())[:10]
    top_10_features_with_price = top_10_features + ['SalePrice']

    # Lọc data
    heatmap_data = temp_df[top_10_features_with_price].copy()

    # Tính correlation matrix
    corr_matrix = heatmap_data.corr()

    # Vẽ heatmap
    plt.figure(figsize=(14, 12))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)

    sns.heatmap(corr_matrix,
                mask=mask,
                annot=True,
                fmt='.3f',
                cmap='RdYlGn',
                center=0,
                square=True,
                linewidths=1,
                cbar_kws={"shrink": 0.8},
                vmin=-1, vmax=1)

    plt.title('Correlation Heatmap - Top 10 New Features + SalePrice',
              fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('correlation_heatmap_new_features.png', dpi=300, bbox_inches='tight')
    plt.show()

# Distribution of new features
if len(existing_new_features) >= 6:
    fig, axes = plt.subplots(3, 2, figsize=(16, 14))
    axes = axes.ravel()

    for i, feature in enumerate(existing_new_features[:6]):
        if feature in all_data.columns:
            # Lọc dữ liệu hợp lệ
            valid_data = all_data[feature].dropna()

            if len(valid_data) > 0:
                axes[i].hist(valid_data, bins=50, color='steelblue',
                           alpha=0.7, edgecolor='black')
                axes[i].set_title(f'Distribution: {feature}',
                                fontsize=12, fontweight='bold')
                axes[i].set_xlabel(feature)
                axes[i].set_ylabel('Frequency')

                # Thêm thống kê
                mean_val = valid_data.mean()
                median_val = valid_data.median()
                axes[i].axvline(mean_val, color='red', linestyle='--',
                              linewidth=2, label=f'Mean: {mean_val:.2f}')
                axes[i].axvline(median_val, color='green', linestyle='--',
                              linewidth=2, label=f'Median: {median_val:.2f}')
                axes[i].legend()

    plt.tight_layout()
    plt.savefig('new_features_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()

"""# III ENCODING VÀ PREPARATION"""

# print("\n=== 5.0 PHÂN TÍCH TOÀN DIỆN DATA TYPES VÀ ENCODING HIỆN TẠI ===")

# # Lưu trạng thái ban đầu để so sánh
# initial_shape = all_data.shape
# initial_memory = all_data.memory_usage(deep=True).sum() / 1024**2

# # PHÂN TÍCH CHI TIẾT CÁC LOẠI DATA TYPES
# print("📊 PHÂN TÍCH DATA TYPES HIỆN TẠI:")
# dtype_analysis = all_data.dtypes.value_counts()
# for dtype, count in dtype_analysis.items():
#     print(f"   {dtype}: {count} columns")

# # PHÂN TÍCH CÁC COLUMNS THEO ĐẶC TÍNH
# print("\n🔍 PHÂN TÍCH CHI TIẾT TỪNG LOẠI COLUMN:")

# # 1. Boolean columns (có thể từ one-hot encoding trước đó)
# bool_cols = all_data.select_dtypes(include=['bool']).columns
# print(f"\n1. BOOLEAN COLUMNS ({len(bool_cols)} columns):")
# if len(bool_cols) > 0:
#     print(f"   Đây có thể là kết quả của one-hot encoding trước đó")
#     print(f"   Ví dụ: {list(bool_cols[:5])}")

# # 2. Category columns
# category_cols = all_data.select_dtypes(include=['category']).columns
# print(f"\n2. CATEGORY COLUMNS ({len(category_cols)} columns):")
# if len(category_cols) > 0:
#     for col in category_cols:
#         unique_vals = all_data[col].nunique()
#         print(f"   - {col}: {unique_vals} categories")

# # 3. Numeric columns với ít unique values (có thể là encoded categorical)
# print(f"\n3. NUMERIC COLUMNS CÓ THỂ LÀ ENCODED CATEGORICAL:")
# numeric_cols = all_data.select_dtypes(include=['int64', 'float64']).columns
# potential_encoded_categorical = []

# for col in numeric_cols:
#     unique_count = all_data[col].nunique()
#     # Nếu là numeric nhưng có ít unique values và không phải là ID/thời gian
#     if (unique_count <= 20 and
#         unique_count > 1 and
#         'ID' not in col and
#         'Year' not in col and
#         'Yr' not in col and
#         'Mo' not in col):
#         potential_encoded_categorical.append({
#             'Column': col,
#             'Dtype': all_data[col].dtype,
#             'Unique_Values': unique_count,
#             'Sample_Values': list(all_data[col].unique()[:5])
#         })

# if len(potential_encoded_categorical) > 0:
#     print(f"   Tìm thấy {len(potential_encoded_categorical)} columns:")
#     potential_df = pd.DataFrame(potential_encoded_categorical)
#     print(potential_df.to_string(index=False))
# else:
#     print("   Không tìm thấy columns nào")

# # HIỂN THỊ BIỂU ĐỒ PHÂN TÍCH
# plt.figure(figsize=(15, 10))

# # Biểu đồ 1: Phân bố data types
# plt.subplot(2, 2, 1)
# dtype_labels = [f'{dtype}\n({count})' for dtype, count in dtype_analysis.items()]
# plt.pie(dtype_analysis.values, labels=dtype_labels, autopct='%1.1f%%', startangle=90)
# plt.title('PHÂN BỐ DATA TYPES HIỆN TẠI')

# # Biểu đồ 2: Phân bố columns theo số lượng unique values
# plt.subplot(2, 2, 2)
# if len(potential_encoded_categorical) > 0:
#     unique_counts = [item['Unique_Values'] for item in potential_encoded_categorical]
#     plt.hist(unique_counts, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
#     plt.xlabel('Số Lượng Unique Values')
#     plt.ylabel('Số Lượng Columns')
#     plt.title('PHÂN BỐ UNIQUE VALUES\n(Trong Potential Categorical)')
# else:
#     plt.text(0.5, 0.5, 'KHÔNG CÓ\nPOTENTIAL CATEGORICAL',
#              ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)
#     plt.title('PHÂN BỐ UNIQUE VALUES')

# # Biểu đồ 3: Memory usage by data type
# plt.subplot(2, 2, 3)
# memory_by_dtype = []
# for dtype in dtype_analysis.index:
#     cols = all_data.select_dtypes(include=[dtype]).columns
#     memory = all_data[cols].memory_usage(deep=True).sum() / 1024**2
#     memory_by_dtype.append(memory)

# plt.bar(range(len(memory_by_dtype)), memory_by_dtype,
#         color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])
# plt.xticks(range(len(memory_by_dtype)), [str(dtype) for dtype in dtype_analysis.index])
# plt.ylabel('Memory Usage (MB)')
# plt.title('MEMORY USAGE THEO DATA TYPE')

# # Biểu đồ 4: Tổng quan encoding status
# plt.subplot(2, 2, 4)
# encoding_status = {
#     'Boolean\n(One-Hot?)': len(bool_cols),
#     'Category': len(category_cols),
#     'Encoded\nCategorical': len(potential_encoded_categorical),
#     'Pure\nNumeric': len(numeric_cols) - len(potential_encoded_categorical)
# }
# colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
# plt.bar(encoding_status.keys(), encoding_status.values(), color=colors, alpha=0.7)
# plt.title('TỔNG QUAN ENCODING STATUS')
# plt.xticks(rotation=45)

# plt.tight_layout()
# plt.show()

# ## 5.1 XỬ LÝ ENCODING CHO CÁC DATA TYPES HIỆN TẠI
# print("\n=== 5.1 XỬ LÝ ENCODING CHO CÁC DATA TYPES HIỆN TẠI ===")

# # Tạo bản sao để xử lý
# all_data_processed = all_data.copy()
# encoding_log = []

# # 1. CHUYỂN BOOLEAN → INTEGER (0,1)
# if len(bool_cols) > 0:
#     print(f"🔧 Chuyển {len(bool_cols)} boolean columns thành integer...")
#     for col in bool_cols:
#         all_data_processed[col] = all_data_processed[col].astype(int)
#         encoding_log.append(f"Boolean → Integer: {col}")
#     print(f"   ✅ Đã chuyển {len(bool_cols)} columns")

# # 2. XỬ LÝ CATEGORY COLUMNS
# if len(category_cols) > 0:
#     print(f"\n🔧 Xử lý {len(category_cols)} category columns...")
#     for col in category_cols:
#         # Kiểm tra nếu là ordered category thì giữ nguyên codes, không thì one-hot
#         if all_data_processed[col].cat.ordered:
#             # Ordered category → chuyển thành numeric codes
#             all_data_processed[col] = all_data_processed[col].cat.codes
#             encoding_log.append(f"Ordered Category → Codes: {col}")
#             print(f"   ✅ {col}: Ordered category → numeric codes")
#         else:
#             # Non-ordered category → one-hot encoding
#             dummies = pd.get_dummies(all_data_processed[col], prefix=col)
#             all_data_processed = pd.concat([all_data_processed.drop(col, axis=1), dummies], axis=1)
#             encoding_log.append(f"Non-Ordered Category → One-Hot: {col}")
#             print(f"   ✅ {col}: Non-ordered category → one-hot encoding ({len(dummies.columns)} features)")
# # 3. XỬ LÝ POTENTIAL ENCODED CATEGORICAL - GIỮ NGUYÊN
# if len(potential_encoded_categorical) > 0:
#     print(f"\n🔧 Giữ nguyên {len(potential_encoded_categorical)} encoded categorical columns...")
#     for item in potential_encoded_categorical:
#         col = item['Column']
#         encoding_log.append(f"Encoded Categorical (Keep): {col}")
#     print(f"   ✅ Đã giữ nguyên {len(potential_encoded_categorical)} columns")

# # CẬP NHẬT all_data
# all_data = all_data_processed

# print("\n=== 5.2 KIỂM TRA VÀ XÁC NHẬN SAU ENCODING ===")

# final_shape = all_data.shape
# final_memory = all_data.memory_usage(deep=True).sum() / 1024**2
# final_dtypes = all_data.dtypes.value_counts()

# print("📊 KẾT QUẢ SAU ENCODING:")
# print(f"   • Shape: {initial_shape} → {final_shape}")
# print(f"   • Memory: {initial_memory:.1f}MB → {final_memory:.1f}MB")
# print(f"   • Data Types cuối cùng:")
# for dtype, count in final_dtypes.items():
#     print(f"     - {dtype}: {count} columns")

# # HIỂN THỊ BIỂU ĐỒ SO SÁNH
# plt.figure(figsize=(12, 5))

# plt.subplot(1, 2, 1)
# # So sánh số lượng features
# metrics = ['Tổng Features', 'Memory (MB)']
# before = [initial_shape[1], initial_memory]
# after = [final_shape[1], final_memory]

# x = np.arange(len(metrics))
# width = 0.35

# bars1 = plt.bar(x - width/2, before, width, label='Trước', color='lightblue', alpha=0.7)
# bars2 = plt.bar(x + width/2, after, width, label='Sau', color='lightgreen', alpha=0.7)

# plt.ylabel('Giá trị')
# plt.title('SO SÁNH TRƯỚC/SAU ENCODING')
# plt.xticks(x, metrics)
# plt.legend()

# # Thêm số liệu
# for i, (b, a) in enumerate(zip(before, after)):
#     plt.text(i - width/2, b + max(before)*0.01, f'{b}', ha='center', va='bottom')
#     plt.text(i + width/2, a + max(after)*0.01, f'{a:.1f}', ha='center', va='bottom')

# plt.subplot(1, 2, 2)
# # Phân bố data types cuối cùng
# dtype_labels = [f'{dtype}\n({count})' for dtype, count in final_dtypes.items()]
# plt.pie(final_dtypes.values, labels=dtype_labels, autopct='%1.1f%%', startangle=90)
# plt.title('DATA TYPES CUỐI CÙNG')

# plt.tight_layout()
# plt.show()

"""## 5.1 PHÂN TÍCH CATEGORICAL FEATURES TRƯỚC KHI ENCODING"""

# 5.1 PHÂN TÍCH CATEGORICAL FEATURES TRƯỚC KHI ENCODING
print("\n=== PHÂN TÍCH CATEGORICAL FEATURES ===")

# Tìm tất cả categorical features còn lại (sau feature engineering)
categorical_cols = all_data.select_dtypes(include=['object']).columns
print(f"Tổng số categorical features cần encoding: {len(categorical_cols)}")

if len(categorical_cols) > 0:
    # Phân tích cardinality của categorical features
    cardinality = {}
    for col in categorical_cols:
        cardinality[col] = all_data[col].nunique()

    cardinality_df = pd.DataFrame.from_dict(cardinality, orient='index', columns=['Unique_Values'])
    cardinality_df = cardinality_df.sort_values('Unique_Values', ascending=False)

    print("\nCategorical features cardinality analysis:")
    print(cardinality_df.head(10))

    # Phân loại categorical features
    low_cardinality = cardinality_df[cardinality_df['Unique_Values'] <= 10]
    medium_cardinality = cardinality_df[(cardinality_df['Unique_Values'] > 10) & (cardinality_df['Unique_Values'] <= 20)]
    high_cardinality = cardinality_df[cardinality_df['Unique_Values'] > 20]

    print(f"\n PHÂN LOẠI CATEGORICAL FEATURES:")
    print(f"    Low cardinality (≤10): {len(low_cardinality)} features")
    print(f"    Medium cardinality (11-20): {len(medium_cardinality)} features")
    print(f"    High cardinality (>20): {len(high_cardinality)} features")

    # Hiển thị chi tiết high cardinality features (nếu có)
    if len(high_cardinality) > 0:
        print(f"\n HIGH CARDINALITY FEATURES (cần xem xét đặc biệt):")
        for feature in high_cardinality.index:
            print(f"   - {feature}: {high_cardinality.loc[feature, 'Unique_Values']} categories")

"""## 5.2 ONE-HOT ENCODING"""

# 5.2 ONE-HOT ENCODING
print("\n=== APPLYING ONE-HOT ENCODING ===")

# Convert any remaining object columns to category type for one-hot encoding
categorical_cols = all_data.select_dtypes(include=['object']).columns

print(f"Shape trước encoding: {all_data.shape}")

#  Kiểm tra memory usage trước encoding
memory_before = all_data.memory_usage(deep=True).sum() / 1024**2  # MB
print(f"Memory usage trước encoding: {memory_before:.2f} MB")

# THỰC HIỆN ONE-HOT ENCODING
all_data = pd.get_dummies(all_data, columns=categorical_cols, drop_first=True)

#  Kiểm tra sau encoding
memory_after = all_data.memory_usage(deep=True).sum() / 1024**2  # MB
print(f"Shape sau encoding: {all_data.shape}")
print(f"Memory usage sau encoding: {memory_after:.2f} MB")
print(f"Số lượng features tăng thêm: {all_data.shape[1] - (all_data.shape[1] - len(categorical_cols))}")

"""## 5.3 KIỂM TRA DATA QUALITY SAU ENCODING"""

# 5.3 KIỂM TRA DATA QUALITY SAU ENCODING
print("\n=== KIỂM TRA CHẤT LƯỢNG DỮ LIỆU ===")
# 1. Tìm chính xác cột có giá trị thiếu
missing_col = all_data.isnull().sum()
missing_col = missing_col[missing_col > 0].index.tolist()

if missing_col:
    col_name = missing_col[0]

    # 2. Kiểm tra kiểu dữ liệu (sau encoding, hầu hết là số)
    if all_data[col_name].dtype.kind in 'fi': # f: float, i: integer
        # Dùng Median (Trung vị) vì nó mạnh mẽ hơn Mean trước outliers
        median_value = all_data[col_name].median()
        all_data[col_name] = all_data[col_name].fillna(median_value)
    else:
        # Dùng Mode nếu là kiểu dữ liệu khác
        mode_value = all_data[col_name].mode()[0]
        all_data[col_name] = all_data[col_name].fillna(mode_value)
        print(f" Đã xử lý giá trị thiếu duy nhất trong cột '{col_name}' bằng Mode: {mode_value}")


# Kiểm tra missing values
print(f"Tổng số giá trị bị thiếu sau encoding: {all_data.isnull().sum().sum()}")

# Kiểm tra infinite values
numerical_data = all_data.select_dtypes(include=[np.number])
print(f"Số lượng giá trị vô hạn: {np.isinf(numerical_data).sum().sum()}")

# Kiểm tra duplicate columns
print(f"Số lượng cột trùng lặp: {len(all_data.columns[all_data.columns.duplicated()])}")

"""## 5.4 CHUẨN BỊ DỮ LIỆU CHO MODELING"""

# 5.4 CHUẨN BỊ DỮ LIỆU CHO MODELING
print("\n=== CHUẨN BỊ DỮ LIỆU CHO MODELING ===")

# Tách dữ liệu đã tiền xử lý thành tập train và test
X = all_data[:len(train_df)]
y = train_df['SalePrice']  # SalePrice đã được biến đổi log và chỉ có trong train_df
X_test_final = all_data[len(train_df):]

print(f"Kích thước X: {X.shape}")
print(f"Kích thước y: {y.shape}")
print(f"Kích thước X_test_final: {X_test_final.shape}")

"""## 5.5 KIỂM TRA FEATURE MATCHING GIỮA TRAIN VÀ TEST"""

# 5.5 KIỂM TRA FEATURE MATCHING GIỮA TRAIN VÀ TEST
print("\n=== KIỂM TRA FEATURE MATCHING ===")

# Đảm bảo train và test có cùng features
if X.shape[1] != X_test_final.shape[1]:
    print(" CẢNH BÁO: Số lượng features trong train và test không khớp!")

    # Tìm features missing
    train_features = set(X.columns)
    test_features = set(X_test_final.columns)

    missing_in_test = train_features - test_features
    missing_in_train = test_features - train_features

    if missing_in_test:
        print(f"Features có trong train nhưng missing trong test: {len(missing_in_test)}")
        # Thêm missing features vào test set với giá trị 0
        for feature in missing_in_test:
            X_test_final[feature] = 0

    if missing_in_train:
        print(f"Features có trong test nhưng missing trong train: {len(missing_in_train)}")
        # Thêm missing features vào train set với giá trị 0
        for feature in missing_in_train:
            X[feature] = 0

    # Sắp xếp columns cho đồng nhất
    X_test_final = X_test_final[X.columns]

    print(" Đã fix feature matching!")
else:
    print(" Train và test features khớp hoàn toàn!")

"""## 5.6 PHÂN TÍCH FINAL FEATURE SET"""

# 5.6 PHÂN TÍCH FINAL FEATURE SET
print("\n=== PHÂN TÍCH FINAL FEATURE SET ===")

print(f"Tổng số features cuối cùng: {X.shape[1]}")
print(f"Số lượng samples training: {X.shape[0]}")
print(f"Số lượng samples testing: {X_test_final.shape[0]}")

# Phân tích feature types
feature_dtypes = X.dtypes.value_counts()
print("\nFeature types distribution:")
for dtype, count in feature_dtypes.items():
    print(f"   {dtype}: {count} features")

"""## 5.7 LƯU TRẠNG THÁI PREPROCESSING"""

# 5.7 LƯU TRẠNG THÁI PREPROCESSING

# Chỉ tính variance cho numerical features, không tính cho categorical
# Chuyển đổi tất cả dữ liệu sang numerical trước khi tính variance
X_numerical = X.select_dtypes(include=[np.number])

if not X_numerical.empty:
    # Tính variance cho từng feature numerical
    variances = X_numerical.var().sort_values()

    print(f"\nFeatures với variance thấp nhất (top 10):")
    for i, (feature, variance) in enumerate(variances.head(10).items()):
        print(f"   {i+1:2d}. {feature:30s}: {variance:.6f}")

    low_variance_features = variances[variances < 0.01]  # Ngưỡng variance thấp
    print(f"\nSố features có variance < 0.01: {len(low_variance_features)}")

    if len(low_variance_features) > 0:
        print("Các features có variance rất thấp (cân nhắc loại bỏ):")
        for feature in low_variance_features.index[:10]:  # Hiển thị top 10
            print(f"   - {feature}")
else:
    print(" Không có numerical features để tính variance")

"""## 5.8 CHIA DỮ LIỆU TRAIN-VALIDATION VÀ CHUẨN BỊ FEATURE SCALING"""

#CHIA DỮ LIỆU TRAIN-VALIDATION VÀ CHUẨN BỊ FEATURE SCALING
# TẠO TRAIN-VALIDATION SPLIT
print("\n=== CHUẨN BỊ TRAIN-VALIDATION SPLIT ===")

# Chia dữ liệu training cho validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Kích thước X_train: {X_train.shape}")
print(f"Kích thước X_val: {X_val.shape}")
print(f"Kích thước y_train: {y_train.shape}")
print(f"Kích thước y_val: {y_val.shape}")

# CHUẨN BỊ FEATURE SCALING (FIXED VERSION)
print("\n=== CHUẨN BỊ FEATURE SCALING ===")

from sklearn.preprocessing import StandardScaler, RobustScaler

# FIX: Chỉ phân tích numerical features cho scaling
numerical_features = X.select_dtypes(include=[np.number]).columns

if len(numerical_features) > 0:
    # Kiểm tra outliers trong numerical features
    print("Phân tích outliers để chọn scaler phù hợp...")

    # Lấy mẫu một số features để kiểm tra (chỉ numerical)
    sample_features = numerical_features[:5] if len(numerical_features) > 5 else numerical_features

    outlier_summary = []
    for feature in sample_features:
        Q1 = X[feature].quantile(0.25)
        Q3 = X[feature].quantile(0.75)
        IQR = Q3 - Q1
        if IQR > 0:  # Tránh chia 0
            outlier_count = ((X[feature] < (Q1 - 1.5 * IQR)) | (X[feature] > (Q3 + 1.5 * IQR))).sum()
            outlier_percent = (outlier_count / len(X)) * 100
            outlier_summary.append((feature, outlier_percent))

            if outlier_percent > 5:  # Nếu có nhiều hơn 5% outliers
                print(f"   {feature}: {outlier_percent:.1f}% outliers -> Nên dùng RobustScaler")
            else:
                print(f"   {feature}: {outlier_percent:.1f}% outliers -> Có thể dùng StandardScaler")
        else:
            print(f"    {feature}: IQR = 0 (hằng số hoặc gần hằng số)")

    # Tính tỷ lệ features có outliers cao
    high_outlier_features = [f for f, p in outlier_summary if p > 5]
    outlier_ratio = len(high_outlier_features) / len(outlier_summary) if outlier_summary else 0

    print(f"\n QUYẾT ĐỊNH SCALING:")
    if outlier_ratio > 0.3:  # Nếu hơn 30% features có outliers cao
        print(f"   Sẽ sử dụng RobustScaler ({outlier_ratio:.1%} features có outliers cao)")
    else:
        print(f"   Sẽ sử dụng StandardScaler ({outlier_ratio:.1%} features có outliers cao)")
else:
    print("Không có numerical features để phân tích scaling")

"""## 5.9 XỬ LÝ CÁC FEATURES CÓ VARIANCE THẤ"""

# 5.9 XỬ LÝ CÁC FEATURES CÓ VARIANCE THẤP
print("\n=== XỬ LÝ LOW-VARIANCE FEATURES ===")

# Đếm số features có gần như constant values
if not X_numerical.empty:
    constant_features = []
    for feature in X_numerical.columns:
        unique_vals = X_numerical[feature].nunique()
        if unique_vals <= 1:
            constant_features.append(feature)

    if constant_features:
        print(f"Tìm thấy {len(constant_features)} features constant (có thể loại bỏ):")
        for feature in constant_features[:10]:  # Hiển thị tối đa 10
            print(f"   - {feature}")
    else:
        print(" Không có features constant")
else:
    print(" Không có numerical features để kiểm tra constant features")

missing_col = all_data.isnull().sum()
missing_col = missing_col[missing_col > 0]

if len(missing_col) > 0:
    print(f"Feature còn missing value: {missing_col.index[0]}")
    all_data[missing_col.index[0]] = all_data[missing_col.index[0]].fillna(all_data[missing_col.index[0]].mode()[0])
else:
    print(" Không còn missing values")

# TỔNG KẾT GIAI ĐOẠN PREPARATION
print("\n" + "="*60)
print(" TỔNG KẾT ENCODING & PREPARATION")
print("="*60)

summary_stats = {
    "Tổng số features": X.shape[1],
    "Training samples": X.shape[0],
    "Test samples": X_test_final.shape[0],
    "Validation samples": X_val.shape[0],
    "Missing values": all_data.isnull().sum().sum(),
    "Memory usage (MB)": f"{memory_after:.1f}",
    "Categorical features encoded": len(categorical_cols),
    "Low-variance features": len(low_variance_features) if 'low_variance_features' in locals() else 0
}

print("THỐNG KÊ CUỐI CÙNG:")
for key, value in summary_stats.items():
    print(f"   {key}: {value}")

"""# MODEL BUILDING & TRAINING

## CHUẨN BỊ DỮ LIỆU CHO MODELING
"""

X_train_final = X_train.select_dtypes(include=[np.number])
X_val_final = X_val.select_dtypes(include=[np.number])
X_test_final_numeric = X_test_final.select_dtypes(include=[np.number])

print(f"X_train_final shape: {X_train_final.shape}")
print(f"X_val_final shape: {X_val_final.shape}")
print(f"X_test_final_numeric shape: {X_test_final_numeric.shape}")

"""## CHUẨN HÓA DỮ LIỆU (FEATURE SCALING)"""

from sklearn.preprocessing import StandardScaler, RobustScaler

# Khởi tạo scaler
scaler = RobustScaler()

# Chuẩn hóa dữ liệu training và validation
X_train_scaled = scaler.fit_transform(X_train_final)
X_val_scaled = scaler.transform(X_val_final)

# Chuẩn hóa dữ liệu test
X_test_scaled = scaler.transform(X_test_final_numeric)

print(f"Kích thước dữ liệu sau chuẩn hóa:")
print(f"   X_train_scaled: {X_train_scaled.shape}")
print(f"   X_val_scaled: {X_val_scaled.shape}")
print(f"   X_test_scaled: {X_test_scaled.shape}")

"""## ĐỊNH NGHĨA CÁC MODELS"""

from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score, KFold
import time

models_advanced = {
    'Random Forest': RandomForestRegressor(
        n_estimators=500,
        max_depth=25,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        bootstrap=True,
        random_state=42,
        n_jobs=-1
    ),

    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=1000,
        learning_rate=0.02,
        max_depth=6,
        min_samples_split=10,
        min_samples_leaf=4,
        subsample=0.8,
        random_state=42
    ),

   'XGBoost': XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=3,
        min_child_weight=1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    ),

    'LightGBM': LGBMRegressor(
    n_estimators=1500,
    learning_rate=0.025,
    max_depth=8,
    num_leaves=50,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42,
    n_jobs=-1,
    verbose=-1,
    silent=True
)
}

print(f"Đã khởi tạo {len(models_advanced)} mô hình")

for i, (name, model) in enumerate(models_advanced.items(), 1):
    print(f"   {i:2d}. {name}")

"""## HÀM ĐÁNH GIÁ VÀ TRAINING MODELS"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def evaluate_models_advanced(models_dict, X_train, X_val, y_train, y_val, cv_folds=5):
    """
    Đánh giá nâng cao với nhiều metrics và cross-validation
    """
    results = {}
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

    # Models cần scaled data
    scaled_models =['K-Neighbors', 'SVR']

    for name, model in models_dict.items():
        print(f" Training {name}...")
        start_time = time.time()

        try:
            # Chọn dữ liệu phù hợp
            if name in scaled_models:
                X_tr, X_v = X_train_scaled, X_val_scaled
            else:
                X_tr, X_v = X_train_final, X_val_final

            # Train model
            model.fit(X_tr, y_train)
            training_time = time.time() - start_time

            # Predictions
            y_pred = model.predict(X_v)

            # Calculate metrics
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            mae = mean_absolute_error(y_val, y_pred)
            r2 = r2_score(y_val, y_pred)

            # Cross-validation scores
            cv_scores_rmse = cross_val_score(model, X_tr, y_train,
                                           cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
            cv_rmse = np.sqrt(-cv_scores_rmse.mean())
            cv_std = np.sqrt(-cv_scores_rmse).std()

            # Additional metrics
            mape = np.mean(np.abs((y_val - y_pred) / y_val)) * 100
            max_error = np.max(np.abs(y_val - y_pred))

            results[name] = {
                'model': model,
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'cv_rmse': cv_rmse,
                'cv_std': cv_std,
                'mape': mape,
                'max_error': max_error,
                'training_time': training_time,
                'predictions': y_pred
            }

            print(f"   [KẾT QUẢ] {name}:")
            print(f"     - RMSE (Val): {rmse:<.4f}  (So với CV RMSE: {cv_rmse:<.4f})")
            print(f"     - R² (Val):   {r2:<.4f}")
            print(f"     - MAE (Val):  {mae:<.4f}")
            print(f"     - Time:       {training_time:.2f}s")

        except Exception as e:
            print(f"    {name} failed: {e}")
            results[name] = None

    return results

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
def create_comprehensive_comparison(results_dict, y_val):
    """
    Tạo các biểu đồ so sánh toàn diện
    """
    if not results_dict:
        print("No results to visualize")
        return

    # Chuẩn bị dữ liệu cho visualization
    models_list = []
    metrics_data = []
    residuals_data = []

    for name, results in results_dict.items():
        if results is not None:
            models_list.append(name)
            metrics_data.append({
                'Model': name,
                'RMSE': results['rmse'],
                'MAE': results['mae'],
                'R²': results['r2'],
                'CV_RMSE': results['cv_rmse'],
                'Training_Time': results['training_time'],
                'MAPE': results['mape']
            })
            residuals = y_val - results['predictions']
            for res in residuals:
                residuals_data.append({'Model': name, 'Residual': res})

    metrics_df = pd.DataFrame(metrics_data)
    residuals_df = pd.DataFrame(residuals_data)

    # === BIỂU ĐỒ 1: SO SÁNH NHIỀU METRICS ===
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))

    # Màu sắc cho từng model
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF', '#5F27CD']

    # 1. RMSE Comparison (Bar chart)
    axes[0,0].barh(range(len(metrics_data)), [x['RMSE'] for x in metrics_data],
                   color=colors[:len(metrics_data)])
    axes[0,0].set_yticks(range(len(metrics_data)))
    axes[0,0].set_yticklabels(models_list)
    axes[0,0].set_xlabel('RMSE (Lower is Better)')
    axes[0,0].set_title('RMSE Comparison')
    axes[0,0].grid(axis='x', alpha=0.3)

    # 2. R² Comparison (Bar chart)
    axes[0,1].barh(range(len(metrics_data)), [x['R²'] for x in metrics_data],
                   color=colors[:len(metrics_data)])
    axes[0,1].set_yticks(range(len(metrics_data)))
    axes[0,1].set_yticklabels(models_list)
    axes[0,1].set_xlabel('R² Score (Higher is Better)')
    axes[0,1].set_title('R² Score Comparison')
    axes[0,1].grid(axis='x', alpha=0.3)

    # 3. Training Time Comparison
    axes[0,2].barh(range(len(metrics_data)), [x['Training_Time'] for x in metrics_data],
                   color=colors[:len(metrics_data)])
    axes[0,2].set_yticks(range(len(metrics_data)))
    axes[0,2].set_yticklabels(models_list)
    axes[0,2].set_xlabel('Training Time (seconds)')
    axes[0,2].set_title('Training Time Comparison')
    axes[0,2].grid(axis='x', alpha=0.3)

    # 4. ĐÁNH GIÁ OVERFITTING (Validation RMSE vs CV RMSE)
    # Chuẩn bị dữ liệu cho biểu đồ cột nhóm
    metrics_melted = metrics_df.melt(id_vars='Model', value_vars=['RMSE', 'CV_RMSE'], var_name='Metric Type', value_name='Score')
    sns.barplot(ax=axes[1, 0], data=metrics_melted, x='Model', y='Score', hue='Metric Type', palette='coolwarm')
    axes[1, 0].set_title('Độ Ổn Định (Val RMSE vs CV RMSE)')
    axes[1, 0].set_ylabel('RMSE Score')
    axes[1, 0].set_xlabel('')
    axes[1, 0].tick_params(axis='x', rotation=15)
    axes[1, 0].legend(title="Loại Metric")
    axes[1, 0].grid(axis='y', alpha=0.3)

    #5. Lấy model tốt nhất (RMSE thấp nhất)
    best_model_name = metrics_df.sort_values('RMSE').iloc[0]['Model']
    best_results = results_dict[best_model_name]
    best_color = colors[models_list.index(best_model_name)]

    axes[1, 1].scatter(y_val, best_results['predictions'], alpha=0.6, color=best_color, s=30)
    # Đường lý tưởng
    min_val, max_val = y_val.min(), y_val.max()
    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2)
    axes[1, 1].set_xlabel('Giá Trị Thực Tế (Actual)')
    axes[1, 1].set_ylabel('Giá Trị Dự Đoán (Predicted)')
    axes[1, 1].set_title(f'Actual vs Predicted (Chỉ Model Tốt Nhất: {best_model_name})')
    axes[1, 1].grid(alpha=0.3)

    # 6. PHÂN PHỐI LỖI (Residuals Box Plot)
    sns.boxplot(ax=axes[1, 2], data=residuals_df, x='Model', y='Residual', palette=colors)
    axes[1, 2].axhline(y=0, color='black', linestyle='--', linewidth=2)
    axes[1, 2].set_xlabel('')
    axes[1, 2].set_ylabel('Giá trị lỗi (Residuals)')
    axes[1, 2].set_title('Phân Phối Lỗi (Box Plot)')
    axes[1, 2].tick_params(axis='x', rotation=15)
    axes[1, 2].grid(axis='y', alpha=0.3)


    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Chừa chỗ cho suptitle
    plt.show()

    # === BIỂU ĐỒ 2: PERFORMANCE TREND THEO THỜI GIAN ===
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Training time vs RMSE
    for i, model_data in enumerate(metrics_data):
        ax1.scatter(model_data['Training_Time'], model_data['RMSE'],
                   color=colors[i], s=100, label=model_data['Model'])
        ax1.annotate(model_data['Model'],
                    (model_data['Training_Time'], model_data['RMSE']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)

    ax1.set_xlabel('Training Time (seconds)')
    ax1.set_ylabel('RMSE')
    ax1.set_title('Training Time vs RMSE')
    ax1.grid(alpha=0.3)

    # R² vs RMSE
    for i, model_data in enumerate(metrics_data):
        ax2.scatter(model_data['R²'], model_data['RMSE'],
                   color=colors[i], s=100, label=model_data['Model'])
        ax2.annotate(model_data['Model'],
                    (model_data['R²'], model_data['RMSE']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)

    ax2.set_xlabel('R² Score')
    ax2.set_ylabel('RMSE')
    ax2.set_title('R² Score vs RMSE')
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    plt.show()

    return metrics_df

print("\nTRAINING VÀ ĐÁNH GIÁ 4 MODELS NÂNG CAO")
import warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)


advanced_results = evaluate_models_advanced(
    models_advanced,
    X_train_final, X_val_final,
    y_train, y_val,
    cv_folds=5
)


metrics_comparison = create_comprehensive_comparison(advanced_results, y_val)

"""## SO SÁNH CHI TIẾT VÀ CHỌN MODEL TỐT NHẤT"""

# BIẾN TOÀN CỤC ĐỂ LƯU MODEL TỐT NHẤT
global best_model, best_model_name, best_model_performance

if metrics_comparison is not None:
    # Sắp xếp theo RMSE
    metrics_comparison_sorted = metrics_comparison.sort_values('RMSE')

    # Hiển thị với formatting đẹp
    display_df = metrics_comparison_sorted[['Model', 'RMSE', 'R²', 'MAE', 'MAPE', 'Training_Time']].round(4)
    display_df['Rank'] = range(1, len(display_df) + 1)
    display_df = display_df[['Rank', 'Model', 'RMSE', 'R²', 'MAE', 'MAPE', 'Training_Time']]

    print(display_df.to_string(index=False))

    # TÌM VÀ LƯU MODEL TỐT NHẤT
    best_model_name = metrics_comparison_sorted.iloc[0]['Model']
    best_rmse = metrics_comparison_sorted.iloc[0]['RMSE']
    best_r2 = metrics_comparison_sorted.iloc[0]['R²']

    # Lấy đối tượng model thực tế từ advanced_results
    if best_model_name in advanced_results and advanced_results[best_model_name] is not None:
        best_model = advanced_results[best_model_name]['model']
        best_model_performance = {
            'rmse': best_rmse,
            'r2': best_r2,
            'mae': metrics_comparison_sorted.iloc[0]['MAE'],
            'training_time': metrics_comparison_sorted.iloc[0]['Training_Time']
        }

        print(f"  LỰA CHỌN MÔ HÌNH TỐT NHẤT (BASELINE)")
        print(f"  Mô hình:      {best_model_name} ")
        print(f"  RMSE:         {best_rmse:.4f}")
        print(f"  R²:           {best_r2:.4f}")
        print(f"  MAE:          {metrics_comparison_sorted.iloc[0]['MAE']:.4f}")

    else:
        print(f" Không thể lấy model {best_model_name} từ advanced_results")
        best_model = None
        best_model_name = None

    # Đánh giá chất lượng model
    if best_r2 > 0.9:
        rating = " XUẤT SẮC"
    elif best_r2 > 0.8:
        rating = " RẤT TỐT"
    elif best_r2 > 0.7:
        rating = " TỐT"
    elif best_r2 > 0.6:
        rating = " KHÁ"
    else:
        rating = " CẦN CẢI THIỆN"

    print(f"   ĐÁNH GIÁ: {rating}")

else:
    print("Không có model nào được train thành công!")
    best_model = None
    best_model_name = None

#  XÁC NHẬN MODEL TỐT NHẤT ĐÃ SẴN SÀNG CHO TUNING

if best_model is not None and best_model_name is not None:
    print(f" SẴN SÀNG: {best_model_name} đã được chọn và lưu")
    print(f"    Performance: RMSE={best_model_performance['rmse']:.4f}, R²={best_model_performance['r2']:.4f}")
else:
    print(" CHƯA SẴN SÀNG: Không có model tốt nhất được tìm thấy")
    print("    Sử dụng XGBoost làm model mặc định")
    best_model_name = "XGBoost"
    best_model = XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=3,
        random_state=42,
        n_jobs=-1
    )
    print(f"    Đã khởi tạo {best_model_name} mặc định")

"""# HYPERPARAMETER TUNING & MODEL OPTIMIZATION

## CHUẨN BỊ DỮ LIỆU CHO TUNING
"""

# SỬ DỤNG MODEL TỐT NHẤT ĐÃ ĐƯỢC CHỌN TỪ PHẦN TRƯỚC
if 'best_model' in globals() and best_model is not None and 'best_model_name' in globals():
    print(f"Sử dụng model tốt nhất đã được chọn: {best_model_name}")
    if 'best_model_performance' in globals():
        print(f"   Performance hiện tại: RMSE={best_model_performance['rmse']:.4f}, R²={best_model_performance['r2']:.4f}")
else:
    print("Không tìm thấy model tốt nhất, sử dụng XGBoost mặc định")
    best_model_name = "XGBoost"
    best_model = XGBRegressor(objective='reg:squarederror', random_state=42)
    best_model_performance = {'rmse': 0.1404, 'r2': 0.8944}

print(f"Model sẽ được tuning: {best_model_name}")

"""## HYPERPARAMETER TUNING CHUYÊN SÂU"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
import warnings
warnings.filterwarnings('ignore')

# HYPERPARAMETER TUNING CHUYÊN SÂU
print(f"\n HYPERPARAMETER TUNING CHUYÊN SÂU CHO {best_model_name} ")

# PARAMETER GRIDS TỐI ƯU CHO TỪNG MODEL
param_grids_expert = {
    'XGBoost': {
        'n_estimators': [800, 1000, 1200, 1500, 2000],
        'learning_rate': [0.005, 0.01, 0.015, 0.02, 0.025],
        'max_depth': [4, 5, 6, 7, 8],
        'min_child_weight': [1, 2, 3, 4, 5],
        'subsample': [0.7, 0.75, 0.8, 0.85, 0.9],
        'colsample_bytree': [0.7, 0.75, 0.8, 0.85, 0.9],
        'colsample_bylevel': [0.7, 0.8, 0.9, 1.0],
        'reg_alpha': [0, 0.001, 0.01, 0.1, 0.5, 1],
        'reg_lambda': [0.1, 0.5, 1, 1.5, 2, 3],
        'gamma': [0, 0.001, 0.01, 0.1, 0.2, 0.3]
    },

    'LightGBM': {
        'n_estimators': [1000, 1200, 1500, 1800, 2000],
        'learning_rate': [0.005, 0.01, 0.015, 0.02, 0.025],
        'max_depth': [6, 7, 8, 9, 10, -1],  # -1 means no limit
        'num_leaves': [31, 40, 50, 60, 70, 80],
        'min_child_samples': [10, 20, 30, 40, 50],
        'subsample': [0.7, 0.75, 0.8, 0.85, 0.9],
        'colsample_bytree': [0.7, 0.75, 0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.001, 0.01, 0.1, 0.5, 1],
        'reg_lambda': [0.1, 0.5, 1, 1.5, 2, 3]
    },

    'Random Forest': {
        'n_estimators': [400, 600, 800, 1000, 1200],
        'max_depth': [15, 20, 25, 30, None],
        'min_samples_split': [2, 3, 5, 7, 10],
        'min_samples_leaf': [1, 2, 3, 4, 5],
        'max_features': ['sqrt', 'log2', 0.7, 0.8, 0.9],
        'bootstrap': [True, False],
        'max_samples': [0.7, 0.8, 0.9, None]
    },

    'Gradient Boosting': {
        'n_estimators': [800, 1000, 1200, 1500, 2000],
        'learning_rate': [0.005, 0.01, 0.015, 0.02, 0.025],
        'max_depth': [4, 5, 6, 7, 8],
        'min_samples_split': [5, 10, 15, 20, 25],
        'min_samples_leaf': [3, 4, 5, 6, 8],
        'subsample': [0.7, 0.75, 0.8, 0.85, 0.9],
        'max_features': ['sqrt', 'log2', 0.7, 0.8, 0.9],
        'validation_fraction': [0.1, 0.15, 0.2],
        'n_iter_no_change': [5, 10, 15]
    }
}

# Chọn parameter grid phù hợp
if best_model_name in param_grids_expert:
    param_grid = param_grids_expert[best_model_name]
    print(f"Parameter grid chuyên sâu cho {best_model_name}:")
    for param, values in param_grid.items():
        print(f"   {param}: {values}")
else:
    print(f"Không có parameter grid chuyên sâu cho {best_model_name}")
    # Fallback to basic grid
    if best_model_name == 'XGBoost':
        param_grid = {
            'n_estimators': [100, 500, 1000],
            'learning_rate': [0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
    else:
        param_grid = {}
        print("Không thể tuning do thiếu parameter grid")

"""## THỰC HIỆN TUNING CHUYÊN SÂU"""

if param_grid:
    X_tuning = X_train_final
    X_val_tuning = X_val_final
    print(f"Sử dụng dữ liệu 'X_train_final' (không scale) cho {best_model_name} tuning.")

    # Phương pháp 1: HalvingRandomSearchCV (nhanh và hiệu quả)
    print("\n1. Sử dụng HalvingRandomSearchCV")

    halving_search = HalvingRandomSearchCV(
        estimator=best_model,
        param_distributions=param_grid,
        n_candidates='exhaust',
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42,
        verbose=1,
        factor=3,
        aggressive_elimination=False
    )

    start_time_halving = time.time()
    halving_search.fit(X_tuning, y_train)
    halving_time = time.time() - start_time_halving

    best_halving_model = halving_search.best_estimator_
    best_halving_score = halving_search.best_score_
    best_halving_rmse = np.sqrt(-best_halving_score)

    print(f"Best Halving RMSE: {best_halving_rmse:.4f}")

    # Phương pháp 2: RandomizedSearchCV (tìm kiếm rộng hơn)
    print("2. Sử dụng RandomizedSearchCV để tìm kiếm sâu hơn")

   # Lấy best parameters từ HalvingSearch làm baseline
    refined_param_grid = {}
    for param, values in param_grid.items():
        best_value = halving_search.best_params_[param]
        if isinstance(values[0], (int, float)):
            # Tạo range xung quanh best value
            if param == 'learning_rate':
                refined_range = [max(0.001, best_value * 0.5), best_value, min(0.1, best_value * 1.5)]
            elif param == 'n_estimators':
                refined_range = [max(100, int(best_value * 0.7)), best_value, min(3000, int(best_value * 1.3))]
            else:
                refined_range = [best_value * 0.8, best_value, best_value * 1.2] if best_value != 0 else values
            # Đảm bảo các giá trị nằm trong khoảng hợp lệ (ví dụ: không âm)
            refined_param_grid[param] = [v for v in refined_range if v > 0]
        else:
            refined_param_grid[param] = [best_value]

    # Thêm một số giá trị ngẫu nhiên để đa dạng
    for param, values in param_grid.items():
        if len(refined_param_grid.get(param, [])) < 3 and len(values) > 3:
            # Thêm 2 giá trị ngẫu nhiên khác
            other_values = [v for v in values if v not in refined_param_grid.get(param, [])]
            if len(other_values) >= 2:
                if param not in refined_param_grid:
                     refined_param_grid[param] = []
                refined_param_grid[param].extend(np.random.choice(other_values, 2, replace=False))


    random_search = RandomizedSearchCV(
        estimator=best_model,
        param_distributions=refined_param_grid,
        n_iter=30,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )

    start_time_random = time.time()
    random_search.fit(X_tuning, y_train)
    random_time = time.time() - start_time_random

    best_random_model = random_search.best_estimator_
    best_random_score = random_search.best_score_
    best_random_rmse = np.sqrt(-best_random_score)

    print(f"Best Random RMSE: {best_random_rmse:.4f}")

    # CHỌN MODEL TỐT NHẤT TỪ 2 PHƯƠNG PHÁP
    if best_halving_rmse <= best_random_rmse:
        tuned_model = best_halving_model
        best_cv_rmse = best_halving_rmse
        best_params = halving_search.best_params_
        print(f"Chọn model từ HalvingSearch (RMSE: {best_halving_rmse:.4f})")
    else:
        tuned_model = best_random_model
        best_cv_rmse = best_random_rmse
        best_params = random_search.best_params_
        print(f"Chọn model từ RandomizedSearch (RMSE: {best_random_rmse:.4f})")

    total_tuning_time = halving_time + random_time

else:
    print("Không thực hiện tuning do thiếu parameter grid")
    tuned_model = best_model
    best_cv_rmse = best_model_performance['rmse']
    best_params = "No tuning"
    total_tuning_time = 0

"""## ĐÁNH GIÁ TUNED MODEL TRÊN VALIDATION SET"""

# 1. Dự đoán SAU KHI TUNE (trên thang log)
y_pred_tuned_log = tuned_model.predict(X_val_final)

# 2. Quy đổi SAU KHI TUNE về tiền tệ
y_pred_tuned_orig = np.expm1(y_pred_tuned_log)
y_val_orig = np.expm1(y_val)

# Luôn luôn tính toán lại dự đoán GỐC (log) từ model GỐC (best_model)
original_predictions_log = best_model.predict(X_val_final)
original_predictions_orig = np.expm1(original_predictions_log)

# 3. Tính metrics chi tiết SAU KHI TUNE (trên tiền tệ)
rmse_tuned = np.sqrt(mean_squared_error(y_val_orig, y_pred_tuned_orig))
mae_tuned = mean_absolute_error(y_val_orig, y_pred_tuned_orig)
r2_tuned = r2_score(y_val_orig, y_pred_tuned_orig)
mape_tuned = np.mean(np.abs((y_val_orig - y_pred_tuned_orig) / y_val_orig)) * 100

# 4. Tính metrics chi tiết TRƯỚC KHI TUNE (trên tiền tệ)
original_rmse = np.sqrt(mean_squared_error(y_val_orig, original_predictions_orig))
original_mae = mean_absolute_error(y_val_orig, original_predictions_orig)
original_r2 = r2_score(y_val_orig, original_predictions_orig)
original_mape = np.mean(np.abs((y_val_orig - original_predictions_orig) / y_val_orig)) * 100
original_training_time = best_model_performance.get('training_time', 'N/A')

# 5. Tính Cải thiện (Improvement)
rmse_improvement = (original_rmse - rmse_tuned) / original_rmse * 100
mae_improvement = (original_mae - mae_tuned) / original_mae * 100
r2_improvement = (r2_tuned - original_r2) / abs(original_r2) * 100


# HIỂN THỊ KẾT QUẢ CHI TIẾT
print(" BẢNG SO SÁNH HIỆU SUẤT (ĐÃ QUY ĐỔI VỀ THANG ĐO TIỀN TỆ GỐC)")

comparison_details = pd.DataFrame({
    'Metric': ['RMSE ($)', 'MAE ($)', 'R²', 'MAPE (%)', 'Training Time (s)'],
    'Trước Tuning': [
        f"${original_rmse:,.2f}",
        f"${original_mae:,.2f}",
        f"{original_r2:.4f}",
        f"{original_mape:.2f}%",
        original_training_time
    ],
    'Sau Tuning': [
        f"${rmse_tuned:,.2f}",
        f"${mae_tuned:,.2f}",
        f"{r2_tuned:.4f}",
        f"{mape_tuned:.2f}%",
        f"{total_tuning_time:.2f}"
    ],
    'Cải Thiện': [
        f"{rmse_improvement:+.2f}%",
        f"{mae_improvement:+.2f}%",
        f"{r2_improvement:+.2f}%",
        'N/A',
        'N/A'
    ]
})

print(comparison_details.to_string(index=False))

# ĐÁNH GIÁ CHẤT LƯỢNG TUNING
print(f"\n PHÂN TÍCH KẾT QUẢ TUNING:")

if rmse_improvement > 2.0:
    rating = "XUẤT SẮC - Cải thiện rất đáng kể!"
    emoji = ""
elif rmse_improvement > 1.0:
    rating = "RẤT TỐT - Cải thiện đáng kể"
    emoji = ""
elif rmse_improvement > 0.5:
    rating = "TỐT - Có cải thiện"
    emoji = ""
elif rmse_improvement > 0:
    rating = "KHÁ - Cải thiện nhẹ"
    emoji = ""
else:
    rating = "CẦN CẢI THIỆN - Không có cải thiện"
    emoji = ""

print(f"   {emoji} {rating}")
print(f"   Mức độ cải thiện RMSE: {rmse_improvement:+.2f}%")
print(f"   Mức độ cải thiện R²: {r2_improvement:+.2f}%")

#  THÔNG TIN MODEL CUỐI CÙNG
print(f"\n THÔNG TIN MODEL TỐI ƯU:")
print(f"   Tên model: {best_model_name}")
print(f"   RMSE cuối cùng: {rmse_tuned:.4f}")
print(f"   R² cuối cùng: {r2_tuned:.4f}")

# Đánh giá chất lượng model cuối cùng
if r2_tuned > 0.91:
    final_rating = "XUẤT SẮC"
elif r2_tuned > 0.88:
    final_rating = "RẤT TỐT"
elif r2_tuned > 0.85:
    final_rating = "TỐT"
elif r2_tuned > 0.8:
    final_rating = "KHÁ"
else:
    final_rating = "CẦN CẢI THIỆN"

print(f"   Đánh giá cuối cùng: {final_rating}")

# LƯU THÔNG TIN TUNING
tuned_model_performance = {
    'rmse': rmse_tuned,
    'r2': r2_tuned,
    'mae': mae_tuned,
    'mape': mape_tuned,
    'improvement_rmse': rmse_improvement,
    'improvement_r2': r2_improvement,
    'best_params': best_params,
    'tuning_time': total_tuning_time
}

"""## VISUALIZATION SO SÁNH HIỆU SUẤT TRÊN THANG ĐO LOGARIT"""

# 1. Lấy metrics SAU KHI TUNE (log)
if 'y_pred_tuned_log' not in locals():
    y_pred_tuned_log = tuned_model.predict(X_val_final) # Đây là dự đoán log
if 'original_predictions_log' not in locals():
    original_predictions_log = best_model.predict(X_val_final) # Đây là dự đoán log gốc

rmse_tuned_log = np.sqrt(mean_squared_error(y_val, y_pred_tuned_log))
mae_tuned_log = mean_absolute_error(y_val, y_pred_tuned_log)
r2_tuned_log = r2_score(y_val, y_pred_tuned_log)

# 2. Lấy metrics TRƯỚC KHI TUNE (log)
original_rmse_log = best_model_performance['rmse']
original_mae_log = best_model_performance.get('mae', 0.0)
original_r2_log = best_model_performance['r2']

# 3. Tính Cải thiện (log)
rmse_improvement_log = (original_rmse_log - rmse_tuned_log) / original_rmse_log * 100
mae_improvement_log = (original_mae_log - mae_tuned_log) / original_mae_log * 100 if original_mae_log != 0 else 0
r2_improvement_log = (r2_tuned_log - original_r2_log) / abs(original_r2_log) * 100 if original_r2_log != 0 else 0

# --- BƯỚC 2: VẼ BIỂU ĐỒ ---

fig, axes = plt.subplots(2, 2, figsize=(18, 12))
fig.suptitle(f"So Sánh Hiệu Suất (Thang Đo Logarit) - Model: {best_model_name}", fontsize=16)

# 1. So sánh metrics
metrics_comparison_log = {
    'Metric': ['RMSE (log)', 'MAE (log)', 'R² (log)'],
    'Before': [original_rmse_log, original_mae_log, original_r2_log],
    'After': [rmse_tuned_log, mae_tuned_log, r2_tuned_log],
    'Improvement': [rmse_improvement_log, mae_improvement_log, r2_improvement_log]
}
metrics_df_log = pd.DataFrame(metrics_comparison_log)

x_pos_log = np.arange(len(metrics_df_log))
width = 0.35

axes[0,0].bar(x_pos_log - width/2, metrics_df_log['Before'], width, label='Before Tuning', color='#FF6B6B', alpha=0.8)
axes[0,0].bar(x_pos_log + width/2, metrics_df_log['After'], width, label='After Tuning', color='#4ECDC4', alpha=0.8)
axes[0,0].set_xlabel('Metrics')
axes[0,0].set_ylabel('Values (Thang đo log)')
axes[0,0].set_title('So Sánh Hiệu Suất (Log Scale)')
axes[0,0].set_xticks(x_pos_log)
axes[0,0].set_xticklabels(metrics_df_log['Metric'])
axes[0,0].legend()
axes[0,0].grid(axis='y', alpha=0.3)

for i, (before, after) in enumerate(zip(metrics_df_log['Before'], metrics_df_log['After'])):
    axes[0,0].text(i - width/2, before, f'{before:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    axes[0,0].text(i + width/2, after, f'{after:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9, color='#006861')

# 2. Biểu đồ improvement
colors_log = ['#10B981' if x > 0 else '#EF4444' for x in metrics_df_log['Improvement']]
bars_log = axes[0,1].bar(metrics_df_log['Metric'], metrics_df_log['Improvement'], color=colors_log, alpha=0.8)
axes[0,1].set_xlabel('Metrics')
axes[0,1].set_ylabel('Improvement (%)')
axes[0,1].set_title('Mức Độ Cải Thiện (Log Scale)')
axes[0,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
axes[0,1].grid(axis='y', alpha=0.3)

for bar, improvement in zip(bars_log, metrics_df_log['Improvement']):
    height = bar.get_height()
    va = 'bottom' if height >= 0 else 'top'
    offset = 0.1 if height >= 0 else -0.1
    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + offset, f'{improvement:+.2f}%', ha='center', va=va, fontweight='bold', color='black')

# 3. Actual vs Predicted (log scale)
axes[1,0].scatter(y_val, original_predictions_log, alpha=0.4, color='#FF6B6B', label='Before Tuning (Gốc)', s=30)
axes[1,0].scatter(y_val, y_pred_tuned_log, alpha=0.6, color='#4ECDC4', label='After Tuning (Tối ưu)', s=30)
min_val_log, max_val_log = y_val.min(), y_val.max()
axes[1,0].plot([min_val_log, max_val_log], [min_val_log, max_val_log], 'k--', alpha=0.8, linewidth=2, label="Đường lý tưởng")
axes[1,0].set_xlabel('Giá trị Thực Tế (Log Scale)')
axes[1,0].set_ylabel('Giá trị Dự Đoán (Log Scale)')
axes[1,0].set_title('So SÁnh Dự Đoán (Log Scale)')
axes[1,0].legend()
axes[1,0].grid(alpha=0.3)

# 4. Residuals comparison (log scale)
residuals_before_log = y_val - original_predictions_log
residuals_after_log = y_val - y_pred_tuned_log
sns.kdeplot(residuals_before_log, ax=axes[1,1], color='#FF6B6B', label='Before Tuning', shade=True)
sns.kdeplot(residuals_after_log, ax=axes[1,1], color='#4ECDC4', label='After Tuning', shade=True)
axes[1,1].axvline(x=0, color='black', linestyle='--', linewidth=2)
axes[1,1].set_xlabel('Lỗi (Residuals - Log Scale)')
axes[1,1].set_ylabel('Mật độ (Density)')
axes[1,1].set_title('Phân Phối Lỗi (Residuals - Log Scale)')
axes[1,1].legend()
axes[1,1].grid(alpha=0.3)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""## VISUALIZATION SO SÁNH HIỆU SUẤT TRÊN THANG ĐO TIỀN TỆ"""

print(f"\nPHÂN TÍCH KẾT QUẢ TUNING ")
y_pred_tuned = tuned_model.predict(X_val_final)

# Tính metrics sau tuning
rmse_tuned = np.sqrt(mean_squared_error(y_val, y_pred_tuned))
mae_tuned = mean_absolute_error(y_val, y_pred_tuned)
r2_tuned = r2_score(y_val, y_pred_tuned)

# Tính improvements
rmse_improvement = (original_rmse - rmse_tuned) / original_rmse * 100
mae_improvement = (original_mae - mae_tuned) / original_mae * 100
r2_improvement = (r2_tuned - original_r2) / abs(original_r2) * 100

# Lấy predictions gốc để so sánh visualization
if 'original_predictions' not in locals():
    original_predictions = best_model.predict(X_val_final)

# Tạo visualization chi tiết
fig, axes = plt.subplots(2, 2, figsize=(18, 12))
fig.suptitle(f"So Sánh Hiệu Suất (Thang Tiền Tệ) - Model: {best_model_name}", fontsize=16)

# 1. So sánh metrics trước và sau tuning
metrics_comparison = {
    'Metric': ['RMSE', 'MAE', 'R²'],
    'Before': [original_rmse, original_mae, original_r2],
    'After': [rmse_tuned, mae_tuned, r2_tuned],
    'Improvement': [rmse_improvement, mae_improvement, r2_improvement]
}
metrics_df = pd.DataFrame(metrics_comparison)

# Biểu đồ 1: So sánh giá trị metrics
x_pos = np.arange(len(metrics_df))
width = 0.35

axes[0,0].bar(x_pos - width/2, metrics_df['Before'], width, label='Before Tuning',
              color='#FF6B6B', alpha=0.8)
axes[0,0].bar(x_pos + width/2, metrics_df['After'], width, label='After Tuning',
              color='#4ECDC4', alpha=0.8)

axes[0,0].set_xlabel('Metrics')
axes[0,0].set_ylabel('Values (Thang đo tiền tệ)')
axes[0,0].set_title('So Sánh Hiệu Suất: Trước vs Sau Tuning')
axes[0,0].set_xticks(x_pos)
axes[0,0].set_xticklabels(metrics_df['Metric'])
axes[0,0].legend()
axes[0,0].grid(axis='y', alpha=0.3)

# Thêm giá trị lên biểu đồ
for i, (before, after) in enumerate(zip(metrics_df['Before'], metrics_df['After'])):
    axes[0,0].text(i - width/2, before, f'{before:,.2f}',
                   ha='center', va='bottom', fontweight='bold', fontsize=9)
    axes[0,0].text(i + width/2, after, f'{after:,.2f}',
                   ha='center', va='bottom', fontweight='bold', fontsize=9, color='#006861')


# 2. Biểu đồ improvement
colors = ['#10B981' if x > 0 else '#EF4444' for x in metrics_df['Improvement']]
bars = axes[0,1].bar(metrics_df['Metric'], metrics_df['Improvement'],
                   color=colors, alpha=0.8)

axes[0,1].set_xlabel('Metrics')
axes[0,1].set_ylabel('Improvement (%)')
axes[0,1].set_title('Mức Độ Cải Thiện Sau Tuning')
axes[0,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
axes[0,1].grid(axis='y', alpha=0.3)

# Thêm giá trị improvement
for bar, improvement in zip(bars, metrics_df['Improvement']):
    height = bar.get_height()
    va = 'bottom' if height >= 0 else 'top'
    offset = 0.1 if height >= 0 else -0.1
    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + offset,
                   f'{improvement:+.2f}%', ha='center', va=va,
                   fontweight='bold', color='black')

# 3. Actual vs Predicted so sánh
axes[1,0].scatter(y_val_orig, original_predictions_orig,
                  alpha=0.4, color='#FF6B6B', label='Before Tuning (Gốc)', s=30)
axes[1,0].scatter(y_val_orig, y_pred_tuned_orig, alpha=0.6, color='#4ECDC4',
                  label='After Tuning (Tối ưu)', s=30)

# Đường lý tưởng (trên thang tiền tệ)
min_val, max_val = y_val_orig.min(), y_val_orig.max()
axes[1,0].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2, label="Đường lý tưởng")

axes[1,0].set_xlabel('Giá trị Thực Tế (Tiền tệ)')
axes[1,0].set_ylabel('Giá trị Dự Đoán (Tiền tệ)')
axes[1,0].set_title('So Sánh Dự Đoán: Trước vs Sau Tuning')
axes[1,0].legend()
axes[1,0].grid(alpha=0.3)

# 4. Residuals comparison
import seaborn as sns # Cần import seaborn
residuals_before = y_val_orig - original_predictions_orig
residuals_after = y_val_orig - y_pred_tuned_orig

# (Dùng hist như code gốc hoặc kde plot như tôi gợi ý trước)
sns.kdeplot(residuals_before, ax=axes[1,1], color='#FF6B6B', label='Before Tuning', shade=True)
sns.kdeplot(residuals_after, ax=axes[1,1], color='#4ECDC4', label='After Tuning', shade=True)

axes[1,1].axvline(x=0, color='black', linestyle='--', linewidth=2)
axes[1,1].set_xlabel('Lỗi (Residuals - Tiền tệ)')
axes[1,1].set_ylabel('Mật độ (Density)')
axes[1,1].set_title('Phân Phối Lỗi (Residuals): Trước vs Sau Tuning')
axes[1,1].legend()
axes[1,1].grid(alpha=0.3)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""## CROSS-VALIDATION VÀ LƯU MODEL TỐI ƯU"""

from sklearn.model_selection import cross_val_score
import joblib
import os
# CROSS-VALIDATION ĐÁNH GIÁ TUNED MODEL
# Xác định dữ liệu phù hợp cho cross-validation
X_cv = X_train_final

# Cross-validation với RMSE
cv_scores_rmse = cross_val_score(
    tuned_model,
    X_cv,
    y_train, # y_train đang ở thang đo LOG
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)
cv_rmse_scores = np.sqrt(-cv_scores_rmse)

# Cross-validation với R²
cv_scores_r2 = cross_val_score(
    tuned_model,
    X_cv,
    y_train, # y_train đang ở thang đo LOG
    cv=5,
    scoring='r2',
    n_jobs=-1
)

print("\n[Kết quả Cross-Validation (Độ ổn định - THANG ĐO LOG)]")
print(f"   RMSE (log) - 5 folds: {[f'{score:.4f}' for score in cv_rmse_scores]}")
print(f"   RMSE (log) trung bình: {cv_rmse_scores.mean():.4f} (±{cv_rmse_scores.std() * 2:.4f})")
print(f"   R² (log) - 5 folds: {[f'{score:.4f}' for score in cv_scores_r2]}")
print(f"   R² (log) trung bình: {cv_scores_r2.mean():.4f} (±{cv_scores_r2.std() * 2:.4f})")

# Đánh giá độ ổn định
cv_rmse_std = cv_rmse_scores.std()
if cv_rmse_std < 0.01:
    stability = "RẤT ỔN ĐỊNH"
elif cv_rmse_std < 0.02:
    stability = "ỔN ĐỊNH"
elif cv_rmse_std < 0.03:
    stability = "KHÁ ỔN ĐỊNH"
else:
    stability = "CÓ BIẾN ĐỘNG"

print(f"   Độ ổn định: {stability} (std: {cv_rmse_std:.4f})")

# LƯU TUNED MODEL VÀ SCALER
print(f"\n Lưu tuned model và scaler")

if not os.path.exists('models'):
    os.makedirs('models')

model_filename = f'models/best_tuned_{best_model_name}_model.joblib'
joblib.dump(tuned_model, model_filename)
print(f"   Đã lưu tuned model: {model_filename}")

# Lưu thông tin tuning performance (dùng .pkl cho dict)
performance_filename = f'models/tuning_performance_{best_model_name}.pkl'
joblib.dump(tuned_model_performance, performance_filename)
print(f"   Đã lưu thông tin performance: {performance_filename}")

# LƯU SCALER LINH HOẠT - CẢI TIẾN
scaler_found = False
scaler_to_save = None
scaler_name_used = None

scaler_candidates = [
    'scaler_robust', 'robust_scaler', 'scaler', 'standard_scaler',
    'minmax_scaler', 'normalizer'
]

# Tìm scaler trong local scope
for scaler_name in scaler_candidates:
    if scaler_name in locals():
        scaler_to_save = locals()[scaler_name]
        scaler_name_used = scaler_name
        scaler_found = True
        print(f"   Tìm thấy scaler: {scaler_name}")
        break

# Nếu không tìm thấy, tìm trong global scope
if not scaler_found:
    for scaler_name in globals():
        if scaler_name in globals():
            scaler_to_save = globals()[scaler_name]
            scaler_name_used = scaler_name
            scaler_found = True
            print(f"   Tìm thấy scaler (global): {scaler_name}")
            break

# Lưu scaler nếu tìm thấy (dùng .joblib)
if scaler_found and scaler_to_save is not None:
    scaler_filename = f'models/{scaler_name_used}.joblib' # ⚠️ Dùng .joblib
    joblib.dump(scaler_to_save, scaler_filename)
    print(f"   Đã lưu scaler: {scaler_filename}")
else:
    #  Cải thiện thông báo:
    print("   Không tìm thấy scaler 'scaler' hoặc 'scaler_robust' để lưu.")
    print("   (Không sao, vì model tree-based không cần scaler để dự đoán).")


#  LƯU THÔNG TIN FEATURE NAMES (QUAN TRỌNG CHO DEPLOYMENT)
# Lưu tên features
feature_names = list(X_train_final.columns) if 'X_train_final' in locals() else list(X_train.columns)
feature_filename = 'models/feature_names.pkl'
joblib.dump(feature_names, feature_filename)
print(f"   Đã lưu feature names: {feature_filename}")
print(f"      Số lượng features: {len(feature_names)}")
print(f"      Ví dụ features: {feature_names[:5]}...")

# TẠO FILE SUMMARY
summary_info = {
    'model_name': best_model_name,
    'model_type': type(tuned_model).__name__,
    'performance_dollar_scale': {
        'rmse': rmse_tuned,
        'r2': r2_tuned,
        'mae': mae_tuned,
        'mape': mape_tuned
    },
    'improvement_percentage': {
        'rmse_improvement': rmse_improvement,
        'r2_improvement': r2_improvement
    },
    'cv_performance_log_scale': {
        'cv_rmse_mean': cv_rmse_scores.mean(),
        'cv_rmse_std': cv_rmse_scores.std(),
        'cv_r2_mean': cv_scores_r2.mean(),
        'cv_r2_std': cv_scores_r2.std()
    },
    'best_params': best_params,
    'feature_count': len(feature_names),
    'tuning_time_seconds': total_tuning_time,
    'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
}

summary_filename = 'models/model_training_summary.pkl'
joblib.dump(summary_info, summary_filename)
print(f"   Đã lưu training summary: {summary_filename}")

print(f" MODEL TỐT NHẤT: {best_model_name}")

print(f"\n HIỆU SUẤT CUỐI CÙNG (trên Validation - ĐÃ QUY ĐỔI VỀ TIỀN TỆ):")
print(f"   • RMSE: ${rmse_tuned:,.2f} (cải thiện: {rmse_improvement:+.2f}%)")
print(f"   • MAE:  ${mae_tuned:,.2f}")
print(f"   • R²:   {r2_tuned:.4f} (cải thiện: {r2_improvement:+.2f}%)")
print(f"   • MAPE: {mape_tuned:.2f}%")

print(f"\n ĐỘ ỔN ĐỊNH (Cross-Validation trên Train - THANG ĐO LOGARIT):")
print(f"   • RMSE (log): {cv_rmse_scores.mean():.4f} ± {cv_rmse_scores.std() * 2:.4f}")
print(f"   • R² (log):   {cv_scores_r2.mean():.4f} ± {cv_scores_r2.std() * 2:.4f}")

print(f"\n CÁC FILE ĐÃ LƯU (vào thư mục 'models/'):")
print(f"   • Model: {model_filename}")
print(f"   • Scaler: {scaler_filename if scaler_found and scaler_to_save is not None else 'Không lưu (Tree model không cần)'}")
pre_names = 'feature_names.pkl'
print(f"   • Features: {pre_names}")
print(f"   • Performance: {performance_filename}")
print(f"   • Summary: {summary_filename}")

# Đánh giá tổng thể cuối cùng
# (Công thức này kết hợp R2 tiền tệ, CV std log, và % cải thiện -> OK)
final_score = (r2_tuned * 0.6 + (1 - cv_rmse_scores.std()) * 0.2 + (max(0, rmse_improvement)/10 * 0.2)) * 10

if final_score >= 9:
    final_rating = "XUẤT SẮC - Model rất tốt cho deployment"
elif final_score >= 8:
    final_rating = "RẤT TỐT - Sẵn sàng cho production"
elif final_score >= 7:
    final_rating = "TỐT - Có thể sử dụng"
else:
    final_rating = "CẦN CẢI THIỆN - Nên thử phương pháp khác"

"""# DELOY"""

# -*- coding: utf-8 -*-
import gradio as gr
import numpy as np
import pandas as pd
import joblib
from datetime import datetime
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import traceback # Import traceback for detailed error logging

# === Bỏ qua các cảnh báo không cần thiết ===
warnings.filterwarnings('ignore')

# === CONFIGURATION ===
MODEL_DIR = 'models'
MODEL_PATH = os.path.join(MODEL_DIR, 'best_tuned_XGBoost_model.joblib') # Use .joblib
SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.joblib') # Use .joblib
FEATURE_NAMES_PATH = os.path.join(MODEL_DIR, 'feature_names.pkl')

# === LOAD MODEL & FEATURES ===
print("\nĐANG TẢI MODEL VÀ FEATURES...")
try:
    tuned_model = joblib.load(MODEL_PATH)
    print(f" Đã load model thành công từ: {MODEL_PATH}")

    if os.path.exists(FEATURE_NAMES_PATH):
        feature_columns = joblib.load(FEATURE_NAMES_PATH)
        if not isinstance(feature_columns, list):
            feature_columns = list(feature_columns)
        print(f" Đã load feature names: {len(feature_columns)} features")
    elif hasattr(tuned_model, 'feature_names_in_'):
         feature_columns = tuned_model.feature_names_in_.tolist()
         print(f" Lấy feature names từ model: {len(feature_columns)} features")
    else:
        feature_columns = None
        print(" Không tìm thấy file feature names và không thể lấy từ model.")

    # Scaler loading (optional for XGBoost)
    if os.path.exists(SCALER_PATH):
        scaler = joblib.load(SCALER_PATH)
        print(f" Đã load scaler từ: {SCALER_PATH}")
    else:
        scaler = None
        print(" Không tìm thấy scaler (Model tree-based thường không yêu cầu).")

except FileNotFoundError as fnf_error:
     print(f" LỖI FileNotFoundError: Không tìm thấy file '{fnf_error.filename}'.")
     print("   Vui lòng đảm bảo các file cần thiết (.joblib, .pkl) nằm trong thư mục '{MODEL_DIR}'.")
     raise
except Exception as e:
    print(f"❌ LỖI KHÁC KHI LOAD: {e}")
    traceback.print_exc()
    raise

# === FEATURE ENGINEERING ===
def prepare_features(input_data):
    """Chuẩn bị features cho prediction (cho 1 hàng input: dict hoặc Series)."""
    if isinstance(input_data, dict):
        df = pd.DataFrame([input_data])
    elif isinstance(input_data, pd.Series):
        df = pd.DataFrame([input_data.to_dict()])
    else:
        raise TypeError("input_data phải là dict hoặc pandas Series")

    # Helper function with improved NaN/type handling
    def safe_get(col, default=0):
        val = df.get(col, pd.Series([default])).iloc[0]
        if pd.isna(val) or val == '':
             return default
        try:
             # Try converting known numeric types first
             if col in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'MSSubClass', 'OverallQual', 'OverallCond', 'ExterQualScore', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr', 'AgeGroup_Num', 'SeasonSold_Num'] or 'SF' in col or 'Area' in col or 'Frontage' in col:
                  numeric_val = int(pd.to_numeric(val)) # Use int for counts/years/scores
             elif col == 'IsCulDSac':
                  numeric_val = int(bool(val)) # Convert bool/numeric to 0/1
             else:
                  numeric_val = float(pd.to_numeric(val)) # Default to float
             return numeric_val if pd.notna(numeric_val) else default
        except (ValueError, TypeError):
             # print(f"Debug: Could not convert '{val}' for {col}, using default {default}") # Optional debug print
             return default

    # Base features config with defaults
    base_features_config = {
        'OverallQual': 7, 'GrLivArea': 1500, 'GarageCars': 2, 'FullBath': 2,
        'BedroomAbvGr': 3, 'YearBuilt': 2000, 'Fireplaces': 0, 'LotArea': 8000,
        'OverallCond': 5, 'TotalBsmtSF': 1000, '1stFlrSF': None, '2ndFlrSF': None,
        'LotFrontage': None, 'HalfBath': 0, 'BsmtFullBath': 0, 'BsmtHalfBath': 0,
        'YearRemodAdd': None, 'OpenPorchSF': 0, 'EnclosedPorch': 0, 'ScreenPorch': 0,
        'TotRmsAbvGrd': None, 'GarageArea': None, 'ExterQualScore': 4, 'IsCulDSac': 0,
        '3SsnPorch': 0, 'BsmtFinSF1': None, 'BsmtFinSF2': 0, 'BsmtUnfSF': None,
        'GarageYrBlt': None, 'KitchenAbvGr': 1, 'LowQualFinSF': 0, 'MasVnrArea': 0,
        'MiscVal': 0, 'MoSold': 6, 'MSSubClass': 60, 'PoolArea': 0, 'WoodDeckSF': 0,
    }

    # Apply input or default using safe_get for conversion
    for feature, default_val in base_features_config.items():
         # Use safe_get to handle conversion and defaults robustly
         df[feature] = safe_get(feature, default_val)


    # Calculate dependent defaults after initial processing
    df['1stFlrSF'] = safe_get('1stFlrSF', safe_get('GrLivArea') * 0.55)
    # Ensure 2ndFlrSF is consistent and non-negative
    df['2ndFlrSF'] = max(0, safe_get('GrLivArea') - df['1stFlrSF'].iloc[0]) if pd.isna(df['2ndFlrSF'].iloc[0]) else safe_get('2ndFlrSF')
    df['LotFrontage'] = safe_get('LotFrontage', np.sqrt(safe_get('LotArea')))
    df['YearRemodAdd'] = safe_get('YearRemodAdd', safe_get('YearBuilt'))
    df['TotRmsAbvGrd'] = safe_get('TotRmsAbvGrd', int(safe_get('BedroomAbvGr') + safe_get('KitchenAbvGr') + 2)) # More robust default
    df['GarageArea'] = safe_get('GarageArea', safe_get('GarageCars') * 250)
    df['BsmtFinSF1'] = safe_get('BsmtFinSF1', safe_get('TotalBsmtSF') * 0.6)
    calculated_unf = safe_get('TotalBsmtSF') - df['BsmtFinSF1'].iloc[0] - safe_get('BsmtFinSF2')
    df['BsmtUnfSF'] = safe_get('BsmtUnfSF', max(0, calculated_unf))
    df['GarageYrBlt'] = safe_get('GarageYrBlt', safe_get('YearBuilt'))

    # Final type enforcement
    for col, default in base_features_config.items():
         if isinstance(default, int) or col in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'OverallQual', 'OverallCond', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr', 'IsCulDSac']:
              df[col] = df[col].astype(int)
         elif isinstance(default, float):
              df[col] = df[col].astype(float)


    # === ENGINEERED FEATURES ===
    current_year = datetime.now().year
    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']
    df['TotalBath'] = (df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath'])
    df['Age'] = current_year - df['YearBuilt']
    df['OverallGrade'] = df['OverallQual'] * df['OverallCond']
    df['QualCondRatio'] = (df['OverallQual'] + 1) / (df['OverallCond'] + 1e-6) # Avoid division by zero if cond is -1? Unlikely but safe.
    df['TotalPorchSF'] = (df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch'])
    df['LivingAreaRatio'] = df['GrLivArea'] / (df['LotArea'] + 1e-6)
    df['RoomArea'] = df['GrLivArea'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['LivingAreaPerBedroom'] = df['GrLivArea'] / (df['BedroomAbvGr'] + 1e-6) # Avoid 0 bedrooms? Add 1? Model expects +0? Check training. Using +1e-6 for now.
    df['HasBasement'] = (df['TotalBsmtSF'] > 0).astype(int)
    df['HasGarage'] = (df['GarageCars'] > 0).astype(int)
    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)
    df['HasSecondFloor'] = (df['2ndFlrSF'] > 0).astype(int)
    df['IsRemodeled'] = (df['YearRemodAdd'] != df['YearBuilt']).astype(int)
    df['YearsSinceRemodel'] = current_year - df['YearRemodAdd']

    age = df['Age'].iloc[0]
    if age <= 5: df['AgeGroup_Num'] = 1
    elif age <= 15: df['AgeGroup_Num'] = 2
    elif age <= 30: df['AgeGroup_Num'] = 3
    elif age <= 50: df['AgeGroup_Num'] = 4
    elif age <= 100: df['AgeGroup_Num'] = 5
    else: df['AgeGroup_Num'] = 6

    month = int(df['MoSold'].iloc[0])
    if month in [12, 1, 2]: df['SeasonSold_Num'] = 1
    elif month in [3, 4, 5]: df['SeasonSold_Num'] = 2
    elif month in [6, 7, 8]: df['SeasonSold_Num'] = 3
    else: df['SeasonSold_Num'] = 4

    df['QualPerSF'] = df['OverallQual'] / (df['TotalSF'] + 1e-6)
    df['BathPerBedroom'] = df['TotalBath'] / (df['BedroomAbvGr'] + 1e-6) # Again, +1e-6 or +1?
    df['BathroomRatio'] = df['TotalBath'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['BedroomRatio'] = df['BedroomAbvGr'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['GarageCarsPerArea'] = df.apply(lambda row: row['GarageCars'] / (row['GarageArea'] + 1e-6) if row['GarageArea'] > 0 else 0, axis=1)
    df['IsNew'] = (current_year == df['YearBuilt']).astype(int)

    # Ensure required columns exist and are ordered
    if feature_columns is not None:
        missing_cols = set(feature_columns) - set(df.columns)
        for c in missing_cols:
            df[c] = 0 # Add missing cols with 0
        try:
             df = df[feature_columns] # Select and order
        except KeyError as e:
             print(f" Lỗi KeyError khi sắp xếp cột: {e}.")
             raise
    else:
        # This case should ideally not happen if loading worked
        print(" CẢNH BÁO NGHIÊM TRỌNG: feature_columns is None. Không thể đảm bảo input cho model.")
        # Attempt to proceed but results might be wrong

    # Final check and fillna
    if df.isnull().values.any():
        nan_cols = df.columns[df.isnull().any()].tolist()
        print(f" Cảnh báo cuối: Phát hiện NaN trong các cột: {nan_cols}. Sẽ thay thế bằng 0.")
        df.fillna(0, inplace=True)

    return df

# === PREDICTION FUNCTION (SINGLE) ===
def predict_house_price(*inputs):
    """Hàm dự đoán giá nhà (cho 1 hàng input)."""
    global feature_columns
    try:
        input_names = [ # List must match the order of Gradio inputs
            'OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'BedroomAbvGr', 'YearBuilt', 'Fireplaces', 'LotArea',
            'OverallCond', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LotFrontage', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',
            'YearRemodAdd', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'TotRmsAbvGrd', 'GarageArea', 'ExterQualScore', 'IsCulDSac'
        ]

        if len(inputs) != len(input_names):
             raise ValueError(f"Lỗi nội bộ: Số lượng input Gradio ({len(inputs)}) không khớp số feature mong đợi ({len(input_names)}).")

        # Convert inputs robustly
        input_data_list = []
        for i, x in enumerate(inputs):
             input_name = input_names[i]
             try:
                 if input_name == 'IsCulDSac':
                      # Handle checkbox (already bool or None)
                      input_data_list.append(1 if x else 0)
                 elif x in [None, '']:
                      input_data_list.append(np.nan) # Keep missing as NaN for prepare_features
                 else:
                     # Try converting to float, handle errors
                     input_data_list.append(float(x))
             except (ValueError, TypeError):
                  input_data_list.append(np.nan) # Treat invalid input as missing
                  print(f" Cảnh báo input: Không thể chuyển đổi '{x}' cho '{input_name}'. Sẽ dùng giá trị mặc định.")

        input_data = dict(zip(input_names, input_data_list))

        # Create features
        features_df = prepare_features(input_data)

        # Predict
        log_price = tuned_model.predict(features_df)[0]
        predicted_price = np.expm1(log_price) # Convert back to dollar scale

        # Calculate bounds using CV RMSE (log scale)
        rmse_log = 0.1282
        lower_bound = np.expm1(log_price - rmse_log)
        upper_bound = np.expm1(log_price + rmse_log)

        # Get R2 score (dollar scale) from validation
        r2_dollar = 0.8992

        # Generate HTML output (without classification)
        result_html = create_result_html(
            predicted_price, lower_bound, upper_bound,
            input_data, r2_dollar, rmse_log # Pass input_data for display
        )
        return result_html

    except Exception as e:
        print(f" Lỗi nghiêm trọng trong predict_house_price: {e}")
        traceback.print_exc()
        return create_error_html(f"Lỗi hệ thống khi dự đoán: {e}")


# === HTML HELPER FUNCTIONS ===
def create_error_html(error_msg):
    """Tạo HTML thông báo lỗi."""
    # (HTML lỗi giữ nguyên)
    return f"""
    <div style="font-family: Arial, sans-serif; text-align: center; padding: 40px; background: #FFF1F1; border-radius: 16px; border: 1px solid #FECACA;">
        <div style="font-size: 3em; margin-bottom: 15px;"></div>
        <h2 style="color: #B91C1C; margin-bottom: 10px;">Ối! Đã có lỗi xảy ra</h2>
        <p style="color: #7F1D1D; font-size: 1em;">
            Không thể thực hiện dự đoán. Vui lòng kiểm tra lại thông tin bạn nhập.
        </p>
        <p style="color: #991B1B; font-size: 0.85em; margin-top: 15px; background: #FEE2E2; padding: 5px; border-radius: 4px;">
            <small>Chi tiết kỹ thuật: {error_msg}</small>
        </p>
    </div>
    """

def create_result_html(predicted_price, lower_bound, upper_bound, input_data, r2_score_val, rmse_log_val):
    """Tạo HTML kết quả (đã bỏ phân loại)."""
    confidence = r2_score_val * 100
    current_year = datetime.now().year
    default_color = "#4f46e5" # Indigo

    # Format input_data for display (handle None/NaN)
    display_input = {}
    for k, v in input_data.items():
        raw_val = input_data.get(k)
        if pd.isna(raw_val):
            display_input[k] = 'Chưa nhập'
        elif k == 'IsCulDSac':
             display_input[k] = 'Có' if int(bool(raw_val)) == 1 else 'Không'
        elif isinstance(raw_val, (int, float)):
             if k in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'OverallQual', 'OverallCond', 'ExterQualScore', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr']:
                 display_input[k] = int(raw_val)
             else:
                  display_input[k] = f"{raw_val:,.0f}"
        else:
            display_input[k] = raw_val

    try:
        yb = display_input.get('YearBuilt')
        age_display = current_year - int(yb) if yb != 'Chưa nhập' and yb != 'N/A' else 'N/A' # Added N/A check
    except:
        age_display = 'N/A'

    # --- CORRECTED HTML F-STRING ---
    return f"""
    <div style="font-family: 'Segoe UI', Arial, sans-serif; max-width: 800px; margin: 20px auto; padding: 10px;">
        <div style="background: linear-gradient(135deg, {default_color}10 0%, {default_color}03 100%);
                    border-radius: 18px; padding: 25px 35px; border: 1px solid {default_color}30;
                    box-shadow: 0 6px 25px rgba(79, 70, 229, 0.07); margin-bottom: 25px;">

            <div style="text-align: center; margin-bottom: 20px;">
                <div style="color: #4B5563; font-size: 0.95em; margin-bottom: 5px;">
                    Giá Trị Nhà Ước Tính
                </div>
                <div style="font-size: 2.8em; font-weight: 700; color: {default_color}; line-height: 1.1;">
                    ${predicted_price:,.0f}
                </div>
            </div>

            <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(4px); border-radius: 10px; padding: 12px 18px; margin-top: 15px; border: 1px solid #e5e7eb;">
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 6px;">
                    <span style="color: #374151; font-weight: 500; font-size: 0.85em;">Độ phù hợp mô hình (R² Validation)</span>
                    <span style="color: {default_color}; font-weight: 600; font-size: 0.95em;">{confidence:.2f}%</span>
                </div>
                <div style="background: #E5E7EB; height: 8px; border-radius: 4px; overflow: hidden;">
                    <div style="background: {default_color};
                                height: 100%; width: {confidence}%;"></div>
                </div>
            </div>
        </div>

        <div style="background: #ffffff; border-radius: 14px; padding: 20px 25px;
                    box-shadow: 0 3px 12px rgba(0, 0, 0, 0.05); border: 1px solid #f3f4f6; margin-bottom: 20px;">
            <h3 style="color: #1F2937; margin-top: 0; margin-bottom: 15px; font-size: 1.05em; font-weight: 600;">
                Khoảng Dao Động Ước Tính (±1 RMSE log)
            </h3>
            <div style="display: flex; justify-content: space-between; gap: 15px;">
                <div style="flex: 1; text-align: center; padding: 12px;
                            background: #FEF2F2;
                            border-radius: 8px; border: 1px solid #FEE2E2;">
                    <div style="color: #7F1D1D; font-size: 0.8em; margin-bottom: 4px; font-weight: 500;">
                        Giá thấp nhất có thể
                    </div>
                    <div style="font-size: 1.4em; font-weight: 600; color: #DC2626;">
                        ${lower_bound:,.0f}
                    </div>
                </div>
                 <div style="flex: 1; text-align: center; padding: 12px;
                            background: #ECFDF5;
                            border-radius: 8px; border: 1px solid #D1FAE5;">
                    <div style="color: #065F46; font-size: 0.8em; margin-bottom: 4px; font-weight: 500;">
                        Giá cao nhất có thể
                    </div>
                    <div style="font-size: 1.4em; font-weight: 600; color: #047857;">
                        ${upper_bound:,.0f}
                    </div>
                </div>
            </div>
             <p style="color: #6B7280; font-size: 0.75em; margin-top: 10px; text-align: center;">
                Dựa trên sai số trung bình (RMSE) = {rmse_log_val:.4f} trên thang logarit (kết quả kiểm tra chéo).
            </p>
        </div>

        <details style="margin-bottom: 15px; background: #F9FAFB; border-radius: 8px; border: 1px solid #E5E7EB; padding: 0;">
          <summary style="cursor: pointer; color: #374151; font-weight: 500; padding: 10px 15px; display: block;">
                <span style="font-size: 0.9em;"> Xem Lại Thông Tin Đầu Vào</span>
          </summary>
          <div style="padding: 15px; border-top: 1px solid #E5E7EB;">
            <dl style="display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 8px 15px; font-size: 0.8em; color: #4B5563; margin: 0;">
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Chất lượng tổng thể:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('OverallQual', 'N/A')}/10</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích sử dụng:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GrLivArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Số phòng ngủ:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('BedroomAbvGr', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm chính:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('FullBath', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm phụ:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('HalfBath', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm hầm:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('BsmtFullBath', 'N/A')} chính, {display_input.get('BsmtHalfBath', 'N/A')} phụ</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Sức chứa gara:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GarageCars', 'N/A')} xe</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích gara:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GarageArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Năm xây dựng:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('YearBuilt', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Năm cải tạo:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('YearRemodAdd', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Tuổi đời:</dt> <dd style="margin-left: 5px; display: inline;">{age_display} năm</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích lô đất:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('LotArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Mặt tiền:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('LotFrontage', 'N/A')} feet</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên trước:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('OpenPorchSF', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên kín:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('EnclosedPorch', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên lưới:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('ScreenPorch', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Đường cụt:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('IsCulDSac', 'N/A')}</dd></div>
            </dl>
          </div>
        </details>

        <div style="text-align: center; color: #A0AEC0; font-size: 0.75em;">
            Dự đoán được thực hiện vào {datetime.now().strftime('%H:%M:%S ngày %d/%m/%Y')}
        </div>
    </div>
    """

# === BATCH PREDICTION FUNCTION ===
# (Giữ nguyên hàm predict_batch như trước)
def predict_batch(uploaded_file):
    """Xử lý file CSV, dự đoán hàng loạt và trả về DataFrame."""
    if uploaded_file is None:
        gr.Warning("Vui lòng tải lên một file CSV.")
        return pd.DataFrame(), gr.DownloadButton.update(visible=False)

    try:
        print(f" Đang đọc file: {uploaded_file.name}")
        try:
            df_input = pd.read_csv(uploaded_file.name)
        except UnicodeDecodeError:
            try:
                df_input = pd.read_csv(uploaded_file.name, encoding='latin1')
                gr.Info("Đã đọc file với encoding 'latin1'.")
            except Exception as read_err:
                 raise ValueError(f"Không thể đọc file CSV. Lỗi encoding hoặc định dạng. {read_err}")

        required_cols = ['OverallQual', 'GrLivArea', 'YearBuilt', 'LotArea']
        missing_required = [col for col in required_cols if col not in df_input.columns]
        if missing_required:
            raise ValueError(f"File CSV thiếu các cột bắt buộc tối thiểu: {', '.join(missing_required)}")

        processed_dfs = []
        valid_indices = [] # Keep track of rows processed successfully
        print(f" Bắt đầu xử lý {len(df_input)} hàng...")
        for index, row in df_input.iterrows():
             try:
                  features_df = prepare_features(row)
                  processed_dfs.append(features_df)
                  valid_indices.append(index) # Record successful index
             except Exception as prep_err:
                  print(f"    Lỗi khi xử lý hàng {index}: {prep_err}. Bỏ qua hàng này.")
             if (index + 1) % 100 == 0:
                  print(f"   Đã xử lý {index + 1}/{len(df_input)} hàng...")

        if not processed_dfs:
             raise ValueError("Không có dữ liệu hợp lệ nào được xử lý từ file.")
        df_processed = pd.concat(processed_dfs, ignore_index=True)

        print(f" Đang dự đoán cho {len(df_processed)} hàng hợp lệ...")
        log_preds = tuned_model.predict(df_processed)
        dollar_preds = np.expm1(log_preds)

        # Create output based on successfully processed rows
        df_output = df_input.iloc[valid_indices].copy() # Select only valid original rows
        # Ensure prediction length matches valid rows count
        if len(df_output) == len(dollar_preds):
             df_output['Predicted_SalePrice'] = np.round(dollar_preds, 0).astype(int)
        else:
             # This case indicates a bug in tracking valid_indices or processing
             print(f" Lỗi nghiêm trọng: Số dự đoán ({len(dollar_preds)}) không khớp số hàng hợp lệ ({len(df_output)}).")
             # Fallback: Just return processed features + preds
             df_output = df_processed.copy()
             df_output['Predicted_SalePrice'] = np.round(dollar_preds, 0).astype(int)
             gr.Warning("Do lỗi xử lý, kết quả có thể không đầy đủ hoặc sai thứ tự.")


        print(" Hoàn thành! Trả về kết quả.")

        output_filename = f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df_output.to_csv(output_filename, index=False)
        gr.Info(f"Xử lý hoàn tất! File kết quả '{output_filename}' đã sẵn sàng để tải về.")

        return df_output, gr.DownloadButton.update(value=output_filename, visible=True)

    except Exception as e:
        print(f" Lỗi khi xử lý file batch: {e}")
        traceback.print_exc()
        gr.Error(f"Lỗi xử lý file: {e}. Vui lòng kiểm tra định dạng và dữ liệu trong file CSV.")
        return pd.DataFrame(), gr.DownloadButton.update(visible=False)

# === FEATURE IMPORTANCE FUNCTION ===
def create_feature_importance_plot():
    """Tạo biểu đồ Feature Importance."""
    global feature_columns, tuned_model
    try:
        if not hasattr(tuned_model, 'feature_importances_'):
             print(" Model không có 'feature_importances_'.")
             fig, ax = plt.subplots(figsize=(10, 6))
             ax.text(0.5, 0.5, 'Model không hỗ trợ Feature Importance.', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='red')
             plt.tight_layout(); return fig

        importances = tuned_model.feature_importances_
        if feature_columns: names = feature_columns
        elif hasattr(tuned_model, 'feature_names_in_'): names = tuned_model.feature_names_in_
        else: names = [f'f_{i}' for i in range(len(importances))]

        if len(names) != len(importances):
             raise ValueError(f"Số lượng tên features ({len(names)}) không khớp importances ({len(importances)}).")

        df_importance = pd.DataFrame({'Feature': names,'Importance': importances}).sort_values(by='Importance', ascending=False)
        df_top = df_importance.head(25)

        print(" Đang tạo biểu đồ Feature Importance...")
        fig, ax = plt.subplots(figsize=(10, 10))
        sns.barplot(data=df_top, x='Importance', y='Feature', palette='viridis_r', ax=ax, orient='h')
        ax.set_title('Top 25 Features Quan Trọng Nhất', fontsize=15, pad=15)
        ax.set_xlabel('Mức độ quan trọng (Score)', fontsize=11)
        ax.set_ylabel('Tên Feature', fontsize=11)
        ax.tick_params(axis='y', labelsize=9)
        ax.grid(axis='x', linestyle='--', alpha=0.6)
        for container in ax.containers: ax.bar_label(container, fmt='%.4f', padding=3, fontsize=8, color='dimgray')
        plt.tight_layout()
        return fig

    except Exception as e:
        print(f"Lỗi khi tạo biểu đồ Feature Importance: {e}")
        traceback.print_exc()
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f'Lỗi tạo biểu đồ:\n{e}', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='red')
        plt.tight_layout(); return fig

# === GRADIO INTERFACE ===
def create_interface():
    """Tạo giao diện Gradio với 3 Tabs"""
    APP_R2 = "89.64%"
    APP_RMSE = "0.1282"
    # Make sure this URL points to a CSV file with headers matching the inputs in Tab 1
    TEMPLATE_URL = "https://raw.githubusercontent.com/hiep2307/House-Prices-Advanced-Regression-Techniques-Kaggle/main/house-prices-advanced-regression-techniques/sample_submission.csv" # Or your own template

    css = """
    #predict-button-single, #predict-button-batch { min-width: 150px; font-size: 1.1em !important; padding: 10px 0px !important; } /* Adjusted padding */
    footer { display: none !important; }
    .gradio-container { max-width: 1320px !important; margin: auto !important; padding-top: 1.5rem !important;} /* Wider container, more padding */
    .gr-prose h1 { font-size: 2.2em !important; }
    .gr-prose p { font-size: 1em !important; } /* Slightly smaller default text */
    .gr-panel { border-radius: 12px !important; box-shadow: 0 2px 8px rgba(0,0,0,0.06) !important; border: 1px solid #e5e7eb !important; } /* Nicer panels */
    .gr-box { border-radius: 10px !important; }
    .gr-tabitem { padding: 15px 20px !important; } /* Add padding to tabs */
    """

    with gr.Blocks(
        title="Dự đoán giá nhà - XGBoost",
        theme=gr.themes.Soft(primary_hue="indigo", secondary_hue="blue"),
        css=css
    ) as demo:

        # --- HEADER ---
        gr.Markdown(f"""
        <div style="text-align: center; padding: 20px 30px;
                    background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%); /* Adjusted Gradient */
                    border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);">
            <h1 style="color: white; font-size: 2.3em; margin-bottom: 5px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);">
                Ứng Dụng Dự Đoán Giá Nhà
            </h1>
            <p style="color: rgba(230, 230, 255, 0.9); font-size: 1em; margin: 0;">
                Model: XGBoost Tuned | CV R² (log): <b>{APP_R2}</b> | CV RMSE (log): <b>{APP_RMSE}</b>
            </p>
        </div>
        """)

        with gr.Tabs():
            # --- TAB 1: DỰ ĐOÁN TỪNG CĂN ---
            with gr.TabItem("Dự đoán Từng Căn"):
                gr.Markdown("Nhập thủ công các thông số của căn nhà để dự đoán giá trị ước tính. Các trường trống sẽ dùng giá trị mặc định.")
                with gr.Row(equal_height=False):
                    # Column 1: Basic Info
                    with gr.Column(scale=1, min_width=280): # Adjusted min_width
                        gr.Markdown("####  Thông tin Cơ bản")
                        overall_qual = gr.Slider(1, 10, value=7, step=1, label="Chất lượng tổng thể")
                        overall_cond = gr.Slider(1, 10, value=5, step=1, label="Tình trạng tổng thể")
                        year_built = gr.Number(value=2005, minimum=1800, maximum=datetime.now().year, step=1, label="Năm xây dựng")
                        year_remod_add = gr.Number(value=2005, minimum=1800, maximum=datetime.now().year, step=1, label="Năm cải tạo")
                        exter_qual_score = gr.Slider(1, 10, value=4, step=1, label="Chất lượng bên ngoài (1-10)") # Corrected range
                        is_cul_d_sac = gr.Checkbox(value=False, label="Nằm trong đường cụt?")

                    # Column 2: Area Info
                    with gr.Column(scale=1, min_width=280):
                        gr.Markdown("####  Diện tích (sqft)")
                        gr_liv_area = gr.Number(value=1800, minimum=300, maximum=6000, label="Diện tích sử dụng")
                        lot_area = gr.Number(value=9000, minimum=1000, maximum=250000, label="Diện tích lô đất")
                        lot_frontage = gr.Number(value=60, minimum=0, maximum=350, label="Mặt tiền (feet)", info="Chiều rộng tiếp giáp đường")
                        total_bsmt_sf = gr.Number(value=1000, minimum=0, maximum=6000, label="Diện tích tầng hầm")
                        first_flr_sf = gr.Number(value=900, minimum=300, maximum=5000, label="Diện tích tầng 1")
                        second_flr_sf = gr.Number(value=0, minimum=0, maximum=4000, label="Diện tích tầng 2") # Adjusted max

                    # Column 3: Rooms & Amenities
                    with gr.Column(scale=1, min_width=280):
                         gr.Markdown("####  Số phòng & Tiện ích")
                         bedroom_abv_gr = gr.Slider(0, 8, value=3, step=1, label="Số phòng ngủ")
                         full_bath = gr.Slider(0, 4, value=2, step=1, label="Phòng tắm chính")
                         half_bath = gr.Slider(0, 2, value=0, step=1, label="Phòng tắm phụ")
                         bsmt_full_bath = gr.Slider(0, 3, value=0, step=1, label="P.tắm hầm (chính)")
                         bsmt_half_bath = gr.Slider(0, 2, value=0, step=1, label="P.tắm hầm (phụ)")
                         tot_rms_abv_grd = gr.Slider(2, 15, value=6, step=1, label="Tổng số phòng")
                         fireplaces = gr.Slider(0, 4, value=1, step=1, label="Số lò sưởi")
                         garage_cars = gr.Slider(0, 5, value=2, step=1, label="Sức chứa gara (xe)")
                         garage_area = gr.Number(value=550, minimum=0, maximum=1500, label="Diện tích gara (sqft)") # Adjusted default
                         open_porch_sf = gr.Number(value=0, minimum=0, maximum=800, label="Hiên trước (sqft)")
                         enclosed_porch = gr.Number(value=0, minimum=0, maximum=600, label="Hiên kín (sqft)")
                         screen_porch = gr.Number(value=0, minimum=0, maximum=600, label="Hiên lưới (sqft)")

                predict_btn = gr.Button(" DỰ ĐOÁN GIÁ", variant="primary", size="lg", elem_id="predict-button-single")
                gr.Markdown("---") # Add more space
                output_single = gr.HTML(label="Kết quả dự đoán")

                # Define inputs_list here for clarity before Examples
                inputs_list = [
                    overall_qual, gr_liv_area, garage_cars, full_bath, bedroom_abv_gr, year_built, fireplaces, lot_area,
                    overall_cond, total_bsmt_sf, first_flr_sf, second_flr_sf, lot_frontage, half_bath, bsmt_full_bath, bsmt_half_bath,
                    year_remod_add, open_porch_sf, enclosed_porch, screen_porch, tot_rms_abv_grd, garage_area, exter_qual_score, is_cul_d_sac
                ]

                gr.Examples(
                     examples=[
                        # Ensure ExterQualScore matches the slider range [1, 10]
                        [9, 3000, 3, 3, 4, 2023, 2, 12000, 8, 1500, 1500, 1500, 100, 1, 1, 0, 2023, 100, 0, 0, 10, 800, 9, True], # Nhà Sang Mới (ExterQual=9)
                        [7, 1800, 2, 2, 3, 1970, 1, 8500,  7, 900,  1200, 600,  65,  1, 0, 0, 2018, 50,  0, 0, 7, 550, 7, False],# Cao Cấp Cải Tạo (ExterQual=7)
                        [6, 1400, 2, 2, 3, 2000, 0, 8000,  6, 700,  700,  700,  60,  0, 0, 0, 2000, 20,  0, 0, 6, 450, 6, False],# Tiêu Chuẩn (ExterQual=6)
                        [4, 900,  1, 1, 2, 1950, 0, 6000,  4, 0,    900,  0,    50,  0, 0, 0, 1950, 0,   50,0, 4, 250, 4, False],# Phổ Thông Cũ (ExterQual=4)
                        [8, 2000, 2, 2, 3, 2015, 1, 7500,  8, 1000, 1000, 1000, 55,  1, 0, 0, 2015, 70,  0, 0, 7, 500, 8, True], # Cao Cấp Đường Cụt (ExterQual=8)
                    ],
                    inputs=inputs_list,
                    label=" Chọn một ví dụ mẫu (Click để thử)",
                    # outputs=output_single, # Can cause issues, let button click handle it
                    # cache_examples=True # Cache examples for faster loading (optional)
                )

                # Simplified explanation section
                gr.Markdown("""
                <details style="margin-top: 20px;">
                  <summary style="cursor: pointer; color: #4A5568; font-weight: 500; font-size: 0.9em;">
                       Giải thích các Ví dụ Mẫu
                  </summary>
                  <div style="background: #F9FAFB; border-radius: 8px; padding: 15px; margin-top: 10px; border: 1px solid #E5E7EB; font-size: 0.85em; color: #4B5563;">
                    <ul>
                        <li><b>Ví dụ 1:</b> Nhà mới, chất lượng rất cao, diện tích lớn, tiện nghi đầy đủ.</li>
                        <li><b>Ví dụ 2:</b> Nhà cũ đã được cải tạo đáng kể, chất lượng tốt.</li>
                        <li><b>Ví dụ 3:</b> Nhà tiêu chuẩn, xây dựng khoảng năm 2000, diện tích vừa phải.</li>
                        <li><b>Ví dụ 4:</b> Nhà nhỏ, cũ, chất lượng thấp, chưa cải tạo.</li>
                        <li><b>Ví dụ 5:</b> Nhà chất lượng tốt, xây gần đây, nằm trong đường cụt yên tĩnh.</li>
                    </ul>
                  </div>
                </details>
                """)

            # --- TAB 2: DỰ ĐOÁN THEO LÔ (FILE) ---
            with gr.TabItem(" Dự đoán Theo Lô (File)"):
                gr.Markdown("Tải lên file `.csv` chứa thông tin nhiều căn nhà để dự đoán hàng loạt. File CSV **phải** có các cột header khớp với tên các trường nhập liệu ở Tab 1 (ví dụ: `OverallQual`, `GrLivArea`, `YearBuilt`...). Các cột thiếu sẽ được gán giá trị mặc định.")
                with gr.Row():
                    with gr.Column(scale=1):
                        file_uploader = gr.File(label="1. Tải lên file CSV", file_types=[".csv"])
                        batch_predict_btn = gr.Button("2. Xử lý File và Dự đoán", variant="primary", elem_id="predict-button-batch")
                        gr.Markdown(f"""
                            <div style="margin-top: 15px; font-size: 0.9em;">
                            **Chưa có file?**
                            <a href="{TEMPLATE_URL}" target="_blank" download="house_price_template.csv" style="color: #4f46e5; text-decoration: none; font-weight: 500;">
                                 Tải file mẫu (.csv) tại đây
                            </a>
                            để xem định dạng cột.
                            </div>
                            """)
                    with gr.Column(scale=3): # Give more space to the table
                        gr.Markdown("### 3. Kết quả dự đoán theo lô")
                        download_btn = gr.DownloadButton(" Tải Kết Quả (.csv)", visible=False) # Add icon
                        batch_output_df = gr.DataFrame(
                            label="Dữ liệu đầu vào và Cột Dự đoán Mới",
                            interactive=False,
                            wrap=True # Allow text wrapping
                        )

            # --- TAB 3: PHÂN TÍCH MÔ HÌNH ---
            with gr.TabItem(" Phân Tích Mô Hình"):
                gr.Markdown("Biểu đồ này hiển thị **mức độ ảnh hưởng** của từng yếu tố đầu vào đến giá nhà dự đoán, theo đánh giá của mô hình XGBoost. Các yếu tố có thanh dài hơn là quan trọng nhất trong việc xác định giá.")
                with gr.Row():
                      refresh_btn = gr.Button("🔄 Tải lại / Cập nhật biểu đồ")
                with gr.Row():
                    importance_plot = gr.Plot(label="Mức độ Quan trọng của Feature (Top 25)")


        # === ĐĂNG KÝ EVENTS (TRIGGERS) ===
        predict_btn.click(
            fn=predict_house_price,
            inputs=inputs_list,
            outputs=output_single,
            api_name="predict_single"
        )
        batch_predict_btn.click(
            fn=predict_batch,
            inputs=[file_uploader],
            outputs=[batch_output_df, download_btn],
            api_name="predict_batch"
        )
        def load_plot():
            print("🔄 Đang tải lại biểu đồ Feature Importance...")
            fig = create_feature_importance_plot()
            if fig is None:
                gr.Warning("Không thể tạo biểu đồ Feature Importance.")
            return fig
        # Load plot initially when app starts
        demo.load(fn=load_plot, inputs=None, outputs=importance_plot)
        # Reload plot when refresh button is clicked
        refresh_btn.click(fn=load_plot, inputs=None, outputs=importance_plot)

    return demo

# === MAIN ===
if __name__ == "__main__":
    print("\n KHỞI ĐỘNG WEB APP DỰ ĐOÁN GIÁ NHÀ...")
    print("--------------------------------------------------")
    print(f" Model:         {MODEL_PATH}")
    print(f" Scaler:        {SCALER_PATH if os.path.exists(SCALER_PATH) else 'Không tìm thấy/Không cần'}")
    print(f" Feature Names: {FEATURE_NAMES_PATH if os.path.exists(FEATURE_NAMES_PATH) else 'Không tìm thấy'}")
    print("--------------------------------------------------")

    # Final check for essential variables
    if 'tuned_model' not in globals():
         print(" Lỗi nghiêm trọng: Biến 'tuned_model' chưa được load. Không thể khởi động App.")
         exit()
    if 'feature_columns' not in globals() or feature_columns is None:
         print(" CẢNH BÁO: Biến 'feature_columns' không được load. Chức năng Feature Engineering có thể không chính xác.")
         # Attempt to load from model again as a last resort
         if hasattr(tuned_model, 'feature_names_in_'):
              feature_columns = tuned_model.feature_names_in_.tolist()
              print("    Đã thử lấy lại feature names từ model.")
         else:
              print("    Không thể lấy feature names từ model. Tiếp tục chạy nhưng có thể gặp lỗi.")
              # Depending on prepare_features robustness, this might still work or fail later

    # Create and launch the interface
    # Tạo giao diện
    demo = create_interface()

    try:
        demo.launch(
            share=True,
            server_name="0.0.0.0",
            server_port=7860,
            show_error=True
        )
        print("\n✅ THÀNH CÔNG! Truy cập qua URL công khai ở trên")
        print("🔗 Hoặc local: http://localhost:7860")

    except Exception as e:
        print(f"❌ LỖI KHI LAUNCH: {e}")
        print("🔄 Thử launch đơn giản hơn...")
        demo.launch(share=False)

# -*- coding: utf-8 -*-
import gradio as gr
import numpy as np
import pandas as pd
import joblib
from datetime import datetime
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import traceback # Import traceback for detailed error logging

# === Bỏ qua các cảnh báo không cần thiết ===
warnings.filterwarnings('ignore')

# === CONFIGURATION ===
MODEL_DIR = 'models'
MODEL_PATH = os.path.join(MODEL_DIR, 'best_tuned_XGBoost_model.joblib') # Use .joblib
SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.joblib') # Use .joblib
FEATURE_NAMES_PATH = os.path.join(MODEL_DIR, 'feature_names.pkl')

# === LOAD MODEL & FEATURES ===
print("\n🔄 ĐANG TẢI MODEL VÀ FEATURES...")
try:
    tuned_model = joblib.load(MODEL_PATH)
    print(f"✅ Đã load model thành công từ: {MODEL_PATH}")

    if os.path.exists(FEATURE_NAMES_PATH):
        feature_columns = joblib.load(FEATURE_NAMES_PATH)
        if not isinstance(feature_columns, list):
            feature_columns = list(feature_columns)
        print(f"✅ Đã load feature names: {len(feature_columns)} features")
    elif hasattr(tuned_model, 'feature_names_in_'):
         feature_columns = tuned_model.feature_names_in_.tolist()
         print(f"✅ Lấy feature names từ model: {len(feature_columns)} features")
    else:
        feature_columns = None
        print("⚠️ Không tìm thấy file feature names và không thể lấy từ model.")

    # Scaler loading (optional for XGBoost)
    if os.path.exists(SCALER_PATH):
        scaler = joblib.load(SCALER_PATH)
        print(f"✅ Đã load scaler từ: {SCALER_PATH}")
    else:
        scaler = None
        print("⚠️ Không tìm thấy scaler (Model tree-based thường không yêu cầu).")

except FileNotFoundError as fnf_error:
     print(f"❌ LỖI FileNotFoundError: Không tìm thấy file '{fnf_error.filename}'.")
     print("   Vui lòng đảm bảo các file cần thiết (.joblib, .pkl) nằm trong thư mục '{MODEL_DIR}'.")
     raise
except Exception as e:
    print(f"❌ LỖI KHÁC KHI LOAD: {e}")
    traceback.print_exc()
    raise

# === FEATURE ENGINEERING ===
def prepare_features(input_data):
    """Chuẩn bị features cho prediction (cho 1 hàng input: dict hoặc Series)."""
    if isinstance(input_data, dict):
        df = pd.DataFrame([input_data])
    elif isinstance(input_data, pd.Series):
        df = pd.DataFrame([input_data.to_dict()])
    else:
        raise TypeError("input_data phải là dict hoặc pandas Series")

    # Helper function with improved NaN/type handling
    def safe_get(col, default=0):
        val = df.get(col, pd.Series([default])).iloc[0]
        if pd.isna(val) or val == '':
             return default
        try:
             # Try converting known numeric types first
             if col in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'MSSubClass', 'OverallQual', 'OverallCond', 'ExterQualScore', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr', 'AgeGroup_Num', 'SeasonSold_Num'] or 'SF' in col or 'Area' in col or 'Frontage' in col:
                  numeric_val = int(pd.to_numeric(val)) # Use int for counts/years/scores
             elif col == 'IsCulDSac':
                  numeric_val = int(bool(val)) # Convert bool/numeric to 0/1
             else:
                  numeric_val = float(pd.to_numeric(val)) # Default to float
             return numeric_val if pd.notna(numeric_val) else default
        except (ValueError, TypeError):
             # print(f"Debug: Could not convert '{val}' for {col}, using default {default}") # Optional debug print
             return default

    # Base features config with defaults
    base_features_config = {
        'OverallQual': 7, 'GrLivArea': 1500, 'GarageCars': 2, 'FullBath': 2,
        'BedroomAbvGr': 3, 'YearBuilt': 2000, 'Fireplaces': 0, 'LotArea': 8000,
        'OverallCond': 5, 'TotalBsmtSF': 1000, '1stFlrSF': None, '2ndFlrSF': None,
        'LotFrontage': None, 'HalfBath': 0, 'BsmtFullBath': 0, 'BsmtHalfBath': 0,
        'YearRemodAdd': None, 'OpenPorchSF': 0, 'EnclosedPorch': 0, 'ScreenPorch': 0,
        'TotRmsAbvGrd': None, 'GarageArea': None, 'ExterQualScore': 4, 'IsCulDSac': 0,
        '3SsnPorch': 0, 'BsmtFinSF1': None, 'BsmtFinSF2': 0, 'BsmtUnfSF': None,
        'GarageYrBlt': None, 'KitchenAbvGr': 1, 'LowQualFinSF': 0, 'MasVnrArea': 0,
        'MiscVal': 0, 'MoSold': 6, 'MSSubClass': 60, 'PoolArea': 0, 'WoodDeckSF': 0,
    }

    # Apply input or default using safe_get for conversion
    for feature, default_val in base_features_config.items():
         # Use safe_get to handle conversion and defaults robustly
         df[feature] = safe_get(feature, default_val)


    # Calculate dependent defaults after initial processing
    df['1stFlrSF'] = safe_get('1stFlrSF', safe_get('GrLivArea') * 0.55)
    # Ensure 2ndFlrSF is consistent and non-negative
    df['2ndFlrSF'] = max(0, safe_get('GrLivArea') - df['1stFlrSF'].iloc[0]) if pd.isna(df['2ndFlrSF'].iloc[0]) else safe_get('2ndFlrSF')
    df['LotFrontage'] = safe_get('LotFrontage', np.sqrt(safe_get('LotArea')))
    df['YearRemodAdd'] = safe_get('YearRemodAdd', safe_get('YearBuilt'))
    df['TotRmsAbvGrd'] = safe_get('TotRmsAbvGrd', int(safe_get('BedroomAbvGr') + safe_get('KitchenAbvGr') + 2)) # More robust default
    df['GarageArea'] = safe_get('GarageArea', safe_get('GarageCars') * 250)
    df['BsmtFinSF1'] = safe_get('BsmtFinSF1', safe_get('TotalBsmtSF') * 0.6)
    calculated_unf = safe_get('TotalBsmtSF') - df['BsmtFinSF1'].iloc[0] - safe_get('BsmtFinSF2')
    df['BsmtUnfSF'] = safe_get('BsmtUnfSF', max(0, calculated_unf))
    df['GarageYrBlt'] = safe_get('GarageYrBlt', safe_get('YearBuilt'))

    # Final type enforcement
    for col, default in base_features_config.items():
         if isinstance(default, int) or col in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'OverallQual', 'OverallCond', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr', 'IsCulDSac']:
              df[col] = df[col].astype(int)
         elif isinstance(default, float):
              df[col] = df[col].astype(float)


    # === ENGINEERED FEATURES ===
    current_year = datetime.now().year
    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']
    df['TotalBath'] = (df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath'])
    df['Age'] = current_year - df['YearBuilt']
    df['OverallGrade'] = df['OverallQual'] * df['OverallCond']
    df['QualCondRatio'] = (df['OverallQual'] + 1) / (df['OverallCond'] + 1e-6) # Avoid division by zero if cond is -1? Unlikely but safe.
    df['TotalPorchSF'] = (df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch'])
    df['LivingAreaRatio'] = df['GrLivArea'] / (df['LotArea'] + 1e-6)
    df['RoomArea'] = df['GrLivArea'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['LivingAreaPerBedroom'] = df['GrLivArea'] / (df['BedroomAbvGr'] + 1e-6) # Avoid 0 bedrooms? Add 1? Model expects +0? Check training. Using +1e-6 for now.
    df['HasBasement'] = (df['TotalBsmtSF'] > 0).astype(int)
    df['HasGarage'] = (df['GarageCars'] > 0).astype(int)
    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)
    df['HasSecondFloor'] = (df['2ndFlrSF'] > 0).astype(int)
    df['IsRemodeled'] = (df['YearRemodAdd'] != df['YearBuilt']).astype(int)
    df['YearsSinceRemodel'] = current_year - df['YearRemodAdd']

    age = df['Age'].iloc[0]
    if age <= 5: df['AgeGroup_Num'] = 1
    elif age <= 15: df['AgeGroup_Num'] = 2
    elif age <= 30: df['AgeGroup_Num'] = 3
    elif age <= 50: df['AgeGroup_Num'] = 4
    elif age <= 100: df['AgeGroup_Num'] = 5
    else: df['AgeGroup_Num'] = 6

    month = int(df['MoSold'].iloc[0])
    if month in [12, 1, 2]: df['SeasonSold_Num'] = 1
    elif month in [3, 4, 5]: df['SeasonSold_Num'] = 2
    elif month in [6, 7, 8]: df['SeasonSold_Num'] = 3
    else: df['SeasonSold_Num'] = 4

    df['QualPerSF'] = df['OverallQual'] / (df['TotalSF'] + 1e-6)
    df['BathPerBedroom'] = df['TotalBath'] / (df['BedroomAbvGr'] + 1e-6) # Again, +1e-6 or +1?
    df['BathroomRatio'] = df['TotalBath'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['BedroomRatio'] = df['BedroomAbvGr'] / (df['TotRmsAbvGrd'] + 1e-6)
    df['GarageCarsPerArea'] = df.apply(lambda row: row['GarageCars'] / (row['GarageArea'] + 1e-6) if row['GarageArea'] > 0 else 0, axis=1)
    df['IsNew'] = (current_year == df['YearBuilt']).astype(int)

    # Ensure required columns exist and are ordered
    if feature_columns is not None:
        missing_cols = set(feature_columns) - set(df.columns)
        for c in missing_cols:
            df[c] = 0 # Add missing cols with 0
        try:
             df = df[feature_columns] # Select and order
        except KeyError as e:
             print(f"❌ Lỗi KeyError khi sắp xếp cột: {e}.")
             raise
    else:
        # This case should ideally not happen if loading worked
        print("🔥 CẢNH BÁO NGHIÊM TRỌNG: feature_columns is None. Không thể đảm bảo input cho model.")
        # Attempt to proceed but results might be wrong

    # Final check and fillna
    if df.isnull().values.any():
        nan_cols = df.columns[df.isnull().any()].tolist()
        print(f"⚠️ Cảnh báo cuối: Phát hiện NaN trong các cột: {nan_cols}. Sẽ thay thế bằng 0.")
        df.fillna(0, inplace=True)

    return df

# === PREDICTION FUNCTION (SINGLE) ===
def predict_house_price(*inputs):
    """Hàm dự đoán giá nhà (cho 1 hàng input)."""
    global feature_columns
    try:
        input_names = [ # List must match the order of Gradio inputs
            'OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'BedroomAbvGr', 'YearBuilt', 'Fireplaces', 'LotArea',
            'OverallCond', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LotFrontage', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',
            'YearRemodAdd', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'TotRmsAbvGrd', 'GarageArea', 'ExterQualScore', 'IsCulDSac'
        ]

        if len(inputs) != len(input_names):
             raise ValueError(f"Lỗi nội bộ: Số lượng input Gradio ({len(inputs)}) không khớp số feature mong đợi ({len(input_names)}).")

        # Convert inputs robustly
        input_data_list = []
        for i, x in enumerate(inputs):
             input_name = input_names[i]
             try:
                 if input_name == 'IsCulDSac':
                      # Handle checkbox (already bool or None)
                      input_data_list.append(1 if x else 0)
                 elif x in [None, '']:
                      input_data_list.append(np.nan) # Keep missing as NaN for prepare_features
                 else:
                     # Try converting to float, handle errors
                     input_data_list.append(float(x))
             except (ValueError, TypeError):
                  input_data_list.append(np.nan) # Treat invalid input as missing
                  print(f"⚠️ Cảnh báo input: Không thể chuyển đổi '{x}' cho '{input_name}'. Sẽ dùng giá trị mặc định.")

        input_data = dict(zip(input_names, input_data_list))

        # Create features
        features_df = prepare_features(input_data)

        # Predict
        log_price = tuned_model.predict(features_df)[0]
        predicted_price = np.expm1(log_price) # Convert back to dollar scale

        # Calculate bounds using CV RMSE (log scale)
        rmse_log = 0.1282
        lower_bound = np.expm1(log_price - rmse_log)
        upper_bound = np.expm1(log_price + rmse_log)

        # Get R2 score (dollar scale) from validation
        r2_dollar = 0.8992

        # Generate HTML output (without classification)
        result_html = create_result_html(
            predicted_price, lower_bound, upper_bound,
            input_data, r2_dollar, rmse_log # Pass input_data for display
        )
        return result_html

    except Exception as e:
        print(f"❌ Lỗi nghiêm trọng trong predict_house_price: {e}")
        traceback.print_exc()
        return create_error_html(f"Lỗi hệ thống khi dự đoán: {e}")


# === HTML HELPER FUNCTIONS ===
def create_error_html(error_msg):
    """Tạo HTML thông báo lỗi."""
    # (HTML lỗi giữ nguyên)
    return f"""
    <div style="font-family: Arial, sans-serif; text-align: center; padding: 40px; background: #FFF1F1; border-radius: 16px; border: 1px solid #FECACA;">
        <div style="font-size: 3em; margin-bottom: 15px;">😥</div>
        <h2 style="color: #B91C1C; margin-bottom: 10px;">Ối! Đã có lỗi xảy ra</h2>
        <p style="color: #7F1D1D; font-size: 1em;">
            Không thể thực hiện dự đoán. Vui lòng kiểm tra lại thông tin bạn nhập.
        </p>
        <p style="color: #991B1B; font-size: 0.85em; margin-top: 15px; background: #FEE2E2; padding: 5px; border-radius: 4px;">
            <small>Chi tiết kỹ thuật: {error_msg}</small>
        </p>
    </div>
    """

def create_result_html(predicted_price, lower_bound, upper_bound, input_data, r2_score_val, rmse_log_val):
    """Tạo HTML kết quả (đã bỏ phân loại)."""
    confidence = r2_score_val * 100
    current_year = datetime.now().year
    default_color = "#4f46e5" # Indigo

    # Format input_data for display (handle None/NaN)
    display_input = {}
    for k, v in input_data.items():
        raw_val = input_data.get(k)
        if pd.isna(raw_val):
            display_input[k] = 'Chưa nhập'
        elif k == 'IsCulDSac':
             display_input[k] = 'Có' if int(bool(raw_val)) == 1 else 'Không'
        elif isinstance(raw_val, (int, float)):
             if k in ['YearBuilt', 'YearRemodAdd', 'MoSold', 'OverallQual', 'OverallCond', 'ExterQualScore', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'KitchenAbvGr']:
                 display_input[k] = int(raw_val)
             else:
                  display_input[k] = f"{raw_val:,.0f}"
        else:
            display_input[k] = raw_val

    try:
        yb = display_input.get('YearBuilt')
        age_display = current_year - int(yb) if yb != 'Chưa nhập' and yb != 'N/A' else 'N/A' # Added N/A check
    except:
        age_display = 'N/A'

    # --- CORRECTED HTML F-STRING ---
    return f"""
    <div style="font-family: 'Segoe UI', Arial, sans-serif; max-width: 800px; margin: 20px auto; padding: 10px;">
        <div style="background: linear-gradient(135deg, {default_color}10 0%, {default_color}03 100%);
                    border-radius: 18px; padding: 25px 35px; border: 1px solid {default_color}30;
                    box-shadow: 0 6px 25px rgba(79, 70, 229, 0.07); margin-bottom: 25px;">

            <div style="text-align: center; margin-bottom: 20px;">
                <div style="color: #4B5563; font-size: 0.95em; margin-bottom: 5px;">
                    Giá Trị Nhà Ước Tính
                </div>
                <div style="font-size: 2.8em; font-weight: 700; color: {default_color}; line-height: 1.1;">
                    ${predicted_price:,.0f}
                </div>
            </div>

            <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(4px); border-radius: 10px; padding: 12px 18px; margin-top: 15px; border: 1px solid #e5e7eb;">
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 6px;">
                    <span style="color: #374151; font-weight: 500; font-size: 0.85em;">Độ phù hợp mô hình (R² Validation)</span>
                    <span style="color: {default_color}; font-weight: 600; font-size: 0.95em;">{confidence:.2f}%</span>
                </div>
                <div style="background: #E5E7EB; height: 8px; border-radius: 4px; overflow: hidden;">
                    <div style="background: {default_color};
                                height: 100%; width: {confidence}%;"></div>
                </div>
            </div>
        </div>

        <div style="background: #ffffff; border-radius: 14px; padding: 20px 25px;
                    box-shadow: 0 3px 12px rgba(0, 0, 0, 0.05); border: 1px solid #f3f4f6; margin-bottom: 20px;">
            <h3 style="color: #1F2937; margin-top: 0; margin-bottom: 15px; font-size: 1.05em; font-weight: 600;">
                Khoảng Dao Động Ước Tính (±1 RMSE log)
            </h3>
            <div style="display: flex; justify-content: space-between; gap: 15px;">
                <div style="flex: 1; text-align: center; padding: 12px;
                            background: #FEF2F2;
                            border-radius: 8px; border: 1px solid #FEE2E2;">
                    <div style="color: #7F1D1D; font-size: 0.8em; margin-bottom: 4px; font-weight: 500;">
                        Giá thấp nhất có thể
                    </div>
                    <div style="font-size: 1.4em; font-weight: 600; color: #DC2626;">
                        ${lower_bound:,.0f}
                    </div>
                </div>
                 <div style="flex: 1; text-align: center; padding: 12px;
                            background: #ECFDF5;
                            border-radius: 8px; border: 1px solid #D1FAE5;">
                    <div style="color: #065F46; font-size: 0.8em; margin-bottom: 4px; font-weight: 500;">
                        Giá cao nhất có thể
                    </div>
                    <div style="font-size: 1.4em; font-weight: 600; color: #047857;">
                        ${upper_bound:,.0f}
                    </div>
                </div>
            </div>
             <p style="color: #6B7280; font-size: 0.75em; margin-top: 10px; text-align: center;">
                Dựa trên sai số trung bình (RMSE) = {rmse_log_val:.4f} trên thang logarit (kết quả kiểm tra chéo).
            </p>
        </div>

        <details style="margin-bottom: 15px; background: #F9FAFB; border-radius: 8px; border: 1px solid #E5E7EB; padding: 0;">
          <summary style="cursor: pointer; color: #374151; font-weight: 500; padding: 10px 15px; display: block;">
                <span style="font-size: 0.9em;">🔍 Xem Lại Thông Tin Đầu Vào</span>
          </summary>
          <div style="padding: 15px; border-top: 1px solid #E5E7EB;">
            <dl style="display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 8px 15px; font-size: 0.8em; color: #4B5563; margin: 0;">
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Chất lượng tổng thể:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('OverallQual', 'N/A')}/10</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích sử dụng:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GrLivArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Số phòng ngủ:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('BedroomAbvGr', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm chính:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('FullBath', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm phụ:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('HalfBath', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Phòng tắm hầm:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('BsmtFullBath', 'N/A')} chính, {display_input.get('BsmtHalfBath', 'N/A')} phụ</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Sức chứa gara:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GarageCars', 'N/A')} xe</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích gara:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('GarageArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Năm xây dựng:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('YearBuilt', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Năm cải tạo:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('YearRemodAdd', 'N/A')}</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Tuổi đời:</dt> <dd style="margin-left: 5px; display: inline;">{age_display} năm</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Diện tích lô đất:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('LotArea', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Mặt tiền:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('LotFrontage', 'N/A')} feet</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên trước:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('OpenPorchSF', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên kín:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('EnclosedPorch', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Hiên lưới:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('ScreenPorch', 'N/A')} sqft</dd></div>
                <div style="margin-bottom: 4px;"><dt style="font-weight: 500; color: #1F2937;">Đường cụt:</dt> <dd style="margin-left: 5px; display: inline;">{display_input.get('IsCulDSac', 'N/A')}</dd></div>
            </dl>
          </div>
        </details>

        <div style="text-align: center; color: #A0AEC0; font-size: 0.75em;">
            Dự đoán được thực hiện vào {datetime.now().strftime('%H:%M:%S ngày %d/%m/%Y')}
        </div>
    </div>
    """

# === BATCH PREDICTION FUNCTION ===
# (Giữ nguyên hàm predict_batch như trước)
def predict_batch(uploaded_file):
    """Xử lý file CSV, dự đoán hàng loạt và trả về DataFrame."""
    if uploaded_file is None:
        gr.Warning("Vui lòng tải lên một file CSV.")
        return pd.DataFrame(), gr.DownloadButton.update(visible=False)

    try:
        print(f"🔄 Đang đọc file: {uploaded_file.name}")
        try:
            df_input = pd.read_csv(uploaded_file.name)
        except UnicodeDecodeError:
            try:
                df_input = pd.read_csv(uploaded_file.name, encoding='latin1')
                gr.Info("Đã đọc file với encoding 'latin1'.")
            except Exception as read_err:
                 raise ValueError(f"Không thể đọc file CSV. Lỗi encoding hoặc định dạng. {read_err}")

        required_cols = ['OverallQual', 'GrLivArea', 'YearBuilt', 'LotArea']
        missing_required = [col for col in required_cols if col not in df_input.columns]
        if missing_required:
            raise ValueError(f"File CSV thiếu các cột bắt buộc tối thiểu: {', '.join(missing_required)}")

        processed_dfs = []
        valid_indices = [] # Keep track of rows processed successfully
        print(f"⏳ Bắt đầu xử lý {len(df_input)} hàng...")
        for index, row in df_input.iterrows():
             try:
                  features_df = prepare_features(row)
                  processed_dfs.append(features_df)
                  valid_indices.append(index) # Record successful index
             except Exception as prep_err:
                  print(f"   ⚠️ Lỗi khi xử lý hàng {index}: {prep_err}. Bỏ qua hàng này.")
             if (index + 1) % 100 == 0:
                  print(f"   Đã xử lý {index + 1}/{len(df_input)} hàng...")

        if not processed_dfs:
             raise ValueError("Không có dữ liệu hợp lệ nào được xử lý từ file.")
        df_processed = pd.concat(processed_dfs, ignore_index=True)

        print(f"🔮 Đang dự đoán cho {len(df_processed)} hàng hợp lệ...")
        log_preds = tuned_model.predict(df_processed)
        dollar_preds = np.expm1(log_preds)

        # Create output based on successfully processed rows
        df_output = df_input.iloc[valid_indices].copy() # Select only valid original rows
        # Ensure prediction length matches valid rows count
        if len(df_output) == len(dollar_preds):
             df_output['Predicted_SalePrice'] = np.round(dollar_preds, 0).astype(int)
        else:
             # This case indicates a bug in tracking valid_indices or processing
             print(f"🔥 Lỗi nghiêm trọng: Số dự đoán ({len(dollar_preds)}) không khớp số hàng hợp lệ ({len(df_output)}).")
             # Fallback: Just return processed features + preds
             df_output = df_processed.copy()
             df_output['Predicted_SalePrice'] = np.round(dollar_preds, 0).astype(int)
             gr.Warning("Do lỗi xử lý, kết quả có thể không đầy đủ hoặc sai thứ tự.")


        print("✅ Hoàn thành! Trả về kết quả.")

        output_filename = f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df_output.to_csv(output_filename, index=False)
        gr.Info(f"Xử lý hoàn tất! File kết quả '{output_filename}' đã sẵn sàng để tải về.")

        return df_output, gr.DownloadButton.update(value=output_filename, visible=True)

    except Exception as e:
        print(f"❌ Lỗi khi xử lý file batch: {e}")
        traceback.print_exc()
        gr.Error(f"Lỗi xử lý file: {e}. Vui lòng kiểm tra định dạng và dữ liệu trong file CSV.")
        return pd.DataFrame(), gr.DownloadButton.update(visible=False)

# === FEATURE IMPORTANCE FUNCTION ===
# (Giữ nguyên hàm create_feature_importance_plot như trước)
def create_feature_importance_plot():
    """Tạo biểu đồ Feature Importance."""
    global feature_columns, tuned_model
    try:
        if not hasattr(tuned_model, 'feature_importances_'):
             print("⚠️ Model không có 'feature_importances_'.")
             fig, ax = plt.subplots(figsize=(10, 6))
             ax.text(0.5, 0.5, 'Model không hỗ trợ Feature Importance.', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='red')
             plt.tight_layout(); return fig

        importances = tuned_model.feature_importances_
        if feature_columns: names = feature_columns
        elif hasattr(tuned_model, 'feature_names_in_'): names = tuned_model.feature_names_in_
        else: names = [f'f_{i}' for i in range(len(importances))]

        if len(names) != len(importances):
             raise ValueError(f"Số lượng tên features ({len(names)}) không khớp importances ({len(importances)}).")

        df_importance = pd.DataFrame({'Feature': names,'Importance': importances}).sort_values(by='Importance', ascending=False)
        df_top = df_importance.head(25)

        print("📊 Đang tạo biểu đồ Feature Importance...")
        fig, ax = plt.subplots(figsize=(10, 10))
        sns.barplot(data=df_top, x='Importance', y='Feature', palette='viridis_r', ax=ax, orient='h')
        ax.set_title('Top 25 Features Quan Trọng Nhất', fontsize=15, pad=15)
        ax.set_xlabel('Mức độ quan trọng (Score)', fontsize=11)
        ax.set_ylabel('Tên Feature', fontsize=11)
        ax.tick_params(axis='y', labelsize=9)
        ax.grid(axis='x', linestyle='--', alpha=0.6)
        for container in ax.containers: ax.bar_label(container, fmt='%.4f', padding=3, fontsize=8, color='dimgray')
        plt.tight_layout()
        return fig

    except Exception as e:
        print(f"❌ Lỗi khi tạo biểu đồ Feature Importance: {e}")
        traceback.print_exc()
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f'Lỗi tạo biểu đồ:\n{e}', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='red')
        plt.tight_layout(); return fig

# === GRADIO INTERFACE ===
def create_interface():
    """Tạo giao diện Gradio với 3 Tabs"""
    APP_R2 = "89.05%"
    APP_RMSE = "0.1282"
    # Make sure this URL points to a CSV file with headers matching the inputs in Tab 1
    TEMPLATE_URL = "https://raw.githubusercontent.com/hiep2307/House-Prices-Advanced-Regression-Techniques-Kaggle/main/house-prices-advanced-regression-techniques/sample_submission.csv" # Or your own template

    css = """
    #predict-button-single, #predict-button-batch { min-width: 150px; font-size: 1.1em !important; padding: 10px 0px !important; } /* Adjusted padding */
    footer { display: none !important; }
    .gradio-container { max-width: 1320px !important; margin: auto !important; padding-top: 1.5rem !important;} /* Wider container, more padding */
    .gr-prose h1 { font-size: 2.2em !important; }
    .gr-prose p { font-size: 1em !important; } /* Slightly smaller default text */
    .gr-panel { border-radius: 12px !important; box-shadow: 0 2px 8px rgba(0,0,0,0.06) !important; border: 1px solid #e5e7eb !important; } /* Nicer panels */
    .gr-box { border-radius: 10px !important; }
    .gr-tabitem { padding: 15px 20px !important; } /* Add padding to tabs */
    """

    with gr.Blocks(
        title="Dự đoán giá nhà - XGBoost",
        theme=gr.themes.Soft(primary_hue="indigo", secondary_hue="blue"),
        css=css
    ) as demo:

        # --- HEADER ---
        gr.Markdown(f"""
        <div style="text-align: center; padding: 20px 30px;
                    background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%); /* Adjusted Gradient */
                    border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);">
            <h1 style="color: white; font-size: 2.3em; margin-bottom: 5px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);">
                Ứng Dụng Dự Đoán Giá Nhà 📈
            </h1>
            <p style="color: rgba(230, 230, 255, 0.9); font-size: 1em; margin: 0;">
                Model: XGBoost Tuned | CV R² (log): <b>{APP_R2}</b> | CV RMSE (log): <b>{APP_RMSE}</b>
            </p>
        </div>
        """)

        with gr.Tabs():
            # --- TAB 1: DỰ ĐOÁN TỪNG CĂN ---
            with gr.TabItem("✍️ Dự đoán Từng Căn"):
                gr.Markdown("Nhập thủ công các thông số của căn nhà để dự đoán giá trị ước tính. Các trường trống sẽ dùng giá trị mặc định.")
                with gr.Row(equal_height=False):
                    # Column 1: Basic Info
                    with gr.Column(scale=1, min_width=280): # Adjusted min_width
                        gr.Markdown("#### 🧱 Thông tin Cơ bản")
                        overall_qual = gr.Slider(1, 10, value=7, step=1, label="Chất lượng tổng thể")
                        overall_cond = gr.Slider(1, 10, value=5, step=1, label="Tình trạng tổng thể")
                        year_built = gr.Number(value=2005, minimum=1800, maximum=datetime.now().year, step=1, label="Năm xây dựng")
                        year_remod_add = gr.Number(value=2005, minimum=1800, maximum=datetime.now().year, step=1, label="Năm cải tạo")
                        exter_qual_score = gr.Slider(1, 10, value=4, step=1, label="Chất lượng bên ngoài (1-10)") # Corrected range
                        is_cul_d_sac = gr.Checkbox(value=False, label="Nằm trong đường cụt?")

                    # Column 2: Area Info
                    with gr.Column(scale=1, min_width=280):
                        gr.Markdown("#### 📐 Diện tích (sqft)")
                        gr_liv_area = gr.Number(value=1800, minimum=300, maximum=6000, label="Diện tích sử dụng")
                        lot_area = gr.Number(value=9000, minimum=1000, maximum=250000, label="Diện tích lô đất")
                        lot_frontage = gr.Number(value=60, minimum=0, maximum=350, label="Mặt tiền (feet)", info="Chiều rộng tiếp giáp đường")
                        total_bsmt_sf = gr.Number(value=1000, minimum=0, maximum=6000, label="Diện tích tầng hầm")
                        first_flr_sf = gr.Number(value=900, minimum=300, maximum=5000, label="Diện tích tầng 1")
                        second_flr_sf = gr.Number(value=0, minimum=0, maximum=4000, label="Diện tích tầng 2") # Adjusted max

                    # Column 3: Rooms & Amenities
                    with gr.Column(scale=1, min_width=280):
                         gr.Markdown("#### 🚪 Số phòng & Tiện ích")
                         bedroom_abv_gr = gr.Slider(0, 8, value=3, step=1, label="Số phòng ngủ")
                         full_bath = gr.Slider(0, 4, value=2, step=1, label="Phòng tắm chính")
                         half_bath = gr.Slider(0, 2, value=0, step=1, label="Phòng tắm phụ")
                         bsmt_full_bath = gr.Slider(0, 3, value=0, step=1, label="P.tắm hầm (chính)")
                         bsmt_half_bath = gr.Slider(0, 2, value=0, step=1, label="P.tắm hầm (phụ)")
                         tot_rms_abv_grd = gr.Slider(2, 15, value=6, step=1, label="Tổng số phòng")
                         fireplaces = gr.Slider(0, 4, value=1, step=1, label="Số lò sưởi")
                         garage_cars = gr.Slider(0, 5, value=2, step=1, label="Sức chứa gara (xe)")
                         garage_area = gr.Number(value=550, minimum=0, maximum=1500, label="Diện tích gara (sqft)") # Adjusted default
                         open_porch_sf = gr.Number(value=0, minimum=0, maximum=800, label="Hiên trước (sqft)")
                         enclosed_porch = gr.Number(value=0, minimum=0, maximum=600, label="Hiên kín (sqft)")
                         screen_porch = gr.Number(value=0, minimum=0, maximum=600, label="Hiên lưới (sqft)")

                predict_btn = gr.Button("💰 DỰ ĐOÁN GIÁ", variant="primary", size="lg", elem_id="predict-button-single")
                gr.Markdown("---") # Add more space
                output_single = gr.HTML(label="Kết quả dự đoán")

                # Define inputs_list here for clarity before Examples
                inputs_list = [
                    overall_qual, gr_liv_area, garage_cars, full_bath, bedroom_abv_gr, year_built, fireplaces, lot_area,
                    overall_cond, total_bsmt_sf, first_flr_sf, second_flr_sf, lot_frontage, half_bath, bsmt_full_bath, bsmt_half_bath,
                    year_remod_add, open_porch_sf, enclosed_porch, screen_porch, tot_rms_abv_grd, garage_area, exter_qual_score, is_cul_d_sac
                ]

                gr.Examples(
                     examples=[
                        # Ensure ExterQualScore matches the slider range [1, 10]
                        [9, 3000, 3, 3, 4, 2023, 2, 12000, 8, 1500, 1500, 1500, 100, 1, 1, 0, 2023, 100, 0, 0, 10, 800, 9, True], # Nhà Sang Mới (ExterQual=9)
                        [7, 1800, 2, 2, 3, 1970, 1, 8500,  7, 900,  1200, 600,  65,  1, 0, 0, 2018, 50,  0, 0, 7, 550, 7, False],# Cao Cấp Cải Tạo (ExterQual=7)
                        [6, 1400, 2, 2, 3, 2000, 0, 8000,  6, 700,  700,  700,  60,  0, 0, 0, 2000, 20,  0, 0, 6, 450, 6, False],# Tiêu Chuẩn (ExterQual=6)
                        [4, 900,  1, 1, 2, 1950, 0, 6000,  4, 0,    900,  0,    50,  0, 0, 0, 1950, 0,   50,0, 4, 250, 4, False],# Phổ Thông Cũ (ExterQual=4)
                        [8, 2000, 2, 2, 3, 2015, 1, 7500,  8, 1000, 1000, 1000, 55,  1, 0, 0, 2015, 70,  0, 0, 7, 500, 8, True], # Cao Cấp Đường Cụt (ExterQual=8)
                    ],
                    inputs=inputs_list,
                    label="✨ Chọn một ví dụ mẫu (Click để thử)",
                    # outputs=output_single, # Can cause issues, let button click handle it
                    # cache_examples=True # Cache examples for faster loading (optional)
                )

                # Simplified explanation section
                gr.Markdown("""
                <details style="margin-top: 20px;">
                  <summary style="cursor: pointer; color: #4A5568; font-weight: 500; font-size: 0.9em;">
                      ℹ️ Giải thích các Ví dụ Mẫu
                  </summary>
                  <div style="background: #F9FAFB; border-radius: 8px; padding: 15px; margin-top: 10px; border: 1px solid #E5E7EB; font-size: 0.85em; color: #4B5563;">
                    <ul>
                        <li><b>Ví dụ 1:</b> Nhà mới, chất lượng rất cao, diện tích lớn, tiện nghi đầy đủ.</li>
                        <li><b>Ví dụ 2:</b> Nhà cũ đã được cải tạo đáng kể, chất lượng tốt.</li>
                        <li><b>Ví dụ 3:</b> Nhà tiêu chuẩn, xây dựng khoảng năm 2000, diện tích vừa phải.</li>
                        <li><b>Ví dụ 4:</b> Nhà nhỏ, cũ, chất lượng thấp, chưa cải tạo.</li>
                        <li><b>Ví dụ 5:</b> Nhà chất lượng tốt, xây gần đây, nằm trong đường cụt yên tĩnh.</li>
                    </ul>
                  </div>
                </details>
                """)

            # --- TAB 2: DỰ ĐOÁN THEO LÔ (MỚI) ---
            with gr.TabItem("📂 Dự đoán Theo Lô (File)"):
                gr.Markdown("Tải lên file `.csv` chứa thông tin nhiều căn nhà để dự đoán hàng loạt. File CSV phải có các cột giống như input (ví dụ: `OverallQual`, `GrLivArea`, `YearBuilt`...).")

                with gr.Row():
                    with gr.Column(scale=1):
                        file_uploader = gr.File(
                            label="1. Tải lên file CSV của bạn", # Thêm số bước
                            file_types=[".csv"]
                        )
                        batch_predict_btn = gr.Button("2. Xử lý File và Dự đoán", variant="primary") # Thêm số bước


                        gr.Markdown(
                            """
                            **Chưa có file?**
                            <a href="https://raw.githubusercontent.com/gradio-app/gradio/main/demo/house_price_prediction/files/house_price_template.csv" target="_blank" download="house_price_template.csv">
                                Tải file mẫu (.csv) tại đây
                            </a>
                            để xem định dạng cột cần thiết.
                            """
                        )
                        # -----------------------------------------------------------------

                    with gr.Column(scale=2):
                        gr.Markdown("### 3. Kết quả dự đoán theo lô") # Thêm số bước
                        download_btn = gr.DownloadButton(
                            label="Tải Kết Quả (.csv)",
                            visible=False
                        )
                        batch_output_df = gr.DataFrame(
                            label="Dữ liệu đầu vào và Dự đoán",
                            interactive=False
                        )

            # --- TAB 3: PHÂN TÍCH MÔ HÌNH (MỚI) ---
            with gr.TabItem("📊 Phân Tích Mô Hình"):
                gr.Markdown("Trực quan hóa các yếu tố quan trọng nhất...")
                importance_plot = gr.Plot(label="Mức độ Quan trọng của Feature (Feature Importance)")
                gr.Markdown("""**Cách đọc biểu đồ:** ...""")

        # === ĐĂNG KÝ EVENTS (TRIGGERS) ===
        # 1. Trigger cho Tab 1
        predict_btn.click(
            fn=predict_house_price,
            inputs=inputs_list,
            outputs=output_single
        )

        # 2. Trigger cho Tab 2
        batch_predict_btn.click(
            fn=predict_batch,
            inputs=[file_uploader],
            outputs=[batch_output_df, download_btn]
        )

        # 3. Trigger cho Tab 3
        demo.load(
            fn=create_feature_importance_plot,
            inputs=None,
            outputs=[importance_plot]
        )

    return demo

# === MAIN (GIỮ NGUYÊN) ===
if __name__ == "__main__":
    print("\n🚀 KHỞI ĐỘNG WEB APP DỰ ĐOÁN GIÁ NHÀ...")
    print("📁 Đường dẫn model:", MODEL_PATH)
    print("📁 Đường dẫn scaler:", SCALER_PATH if os.path.exists(SCALER_PATH) else "Không tìm thấy")
    print("📁 Đường dẫn features:", FEATURE_NAMES_PATH if os.path.exists(FEATURE_NAMES_PATH) else "Không tìm thấy")

    demo = create_interface()

    try:
        demo.launch(
            share=True,
            server_name="0.0.0.0",
            server_port=7860,
            show_error=True
        )
        print("\n✅ THÀNH CÔNG! Truy cập qua URL công khai ở trên")
        print("🔗 Hoặc local: http://localhost:7860")

    except Exception as e:
        print(f"❌ LỖI KHI LAUNCH: {e}")
        print("🔄 Thử launch đơn giản hơn...")
        demo.launch(share=False)